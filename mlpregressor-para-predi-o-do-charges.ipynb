{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Aqui, para fins de estudo, vamos utilizar apenas o algoritmo multilayer perceptron para regressão. Então, com o intuito de propor uma solução para esse problema, vamos seguir as seguintes etapas:\n\n- Carregar e visualizar os primeiros registros do dataset para entender a estrutura;\n- Verificar a necessidade de pré-processamento;\n- Dividir a base de dados em treinamento e teste;\n- Definir e treinar o modelo MLPRegressor;\n- Fazer ponderações sobre os resultados.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\nfile_path = '../input/insurance/insurance.csv'\ndf = pd.read_csv(file_path)\n\n# Primeira 5 linhas do dataset\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-07T20:38:12.204404Z","iopub.execute_input":"2023-11-07T20:38:12.205617Z","iopub.status.idle":"2023-11-07T20:38:12.714979Z","shell.execute_reply.started":"2023-11-07T20:38:12.205489Z","shell.execute_reply":"2023-11-07T20:38:12.713772Z"},"trusted":true},"execution_count":1,"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"   age     sex     bmi  children smoker     region      charges\n0   19  female  27.900         0    yes  southwest  16884.92400\n1   18    male  33.770         1     no  southeast   1725.55230\n2   28    male  33.000         3     no  southeast   4449.46200\n3   33    male  22.705         0     no  northwest  21984.47061\n4   32    male  28.880         0     no  northwest   3866.85520","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>sex</th>\n      <th>bmi</th>\n      <th>children</th>\n      <th>smoker</th>\n      <th>region</th>\n      <th>charges</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>19</td>\n      <td>female</td>\n      <td>27.900</td>\n      <td>0</td>\n      <td>yes</td>\n      <td>southwest</td>\n      <td>16884.92400</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>18</td>\n      <td>male</td>\n      <td>33.770</td>\n      <td>1</td>\n      <td>no</td>\n      <td>southeast</td>\n      <td>1725.55230</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>28</td>\n      <td>male</td>\n      <td>33.000</td>\n      <td>3</td>\n      <td>no</td>\n      <td>southeast</td>\n      <td>4449.46200</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>33</td>\n      <td>male</td>\n      <td>22.705</td>\n      <td>0</td>\n      <td>no</td>\n      <td>northwest</td>\n      <td>21984.47061</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>32</td>\n      <td>male</td>\n      <td>28.880</td>\n      <td>0</td>\n      <td>no</td>\n      <td>northwest</td>\n      <td>3866.85520</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2023-11-07T20:38:12.716769Z","iopub.execute_input":"2023-11-07T20:38:12.717873Z","iopub.status.idle":"2023-11-07T20:38:12.728464Z","shell.execute_reply.started":"2023-11-07T20:38:12.717782Z","shell.execute_reply":"2023-11-07T20:38:12.727166Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"(1338, 7)"},"metadata":{}}]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2023-11-07T20:38:12.730023Z","iopub.execute_input":"2023-11-07T20:38:12.730432Z","iopub.status.idle":"2023-11-07T20:38:12.773197Z","shell.execute_reply.started":"2023-11-07T20:38:12.730390Z","shell.execute_reply":"2023-11-07T20:38:12.771969Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1338 entries, 0 to 1337\nData columns (total 7 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   age       1338 non-null   int64  \n 1   sex       1338 non-null   object \n 2   bmi       1338 non-null   float64\n 3   children  1338 non-null   int64  \n 4   smoker    1338 non-null   object \n 5   region    1338 non-null   object \n 6   charges   1338 non-null   float64\ndtypes: float64(2), int64(2), object(3)\nmemory usage: 73.3+ KB\n","output_type":"stream"}]},{"cell_type":"code","source":"df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-11-07T20:38:12.776276Z","iopub.execute_input":"2023-11-07T20:38:12.776857Z","iopub.status.idle":"2023-11-07T20:38:12.790208Z","shell.execute_reply.started":"2023-11-07T20:38:12.776813Z","shell.execute_reply":"2023-11-07T20:38:12.788063Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"age         0\nsex         0\nbmi         0\nchildren    0\nsmoker      0\nregion      0\ncharges     0\ndtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"Como não tem nenhum dado nulo, já descarta uma possível intervenção no pré-processamento","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2023-11-07T20:38:12.791674Z","iopub.execute_input":"2023-11-07T20:38:12.792161Z","iopub.status.idle":"2023-11-07T20:38:12.828666Z","shell.execute_reply.started":"2023-11-07T20:38:12.792093Z","shell.execute_reply":"2023-11-07T20:38:12.827733Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"               age          bmi     children       charges\ncount  1338.000000  1338.000000  1338.000000   1338.000000\nmean     39.207025    30.663397     1.094918  13270.422265\nstd      14.049960     6.098187     1.205493  12110.011237\nmin      18.000000    15.960000     0.000000   1121.873900\n25%      27.000000    26.296250     0.000000   4740.287150\n50%      39.000000    30.400000     1.000000   9382.033000\n75%      51.000000    34.693750     2.000000  16639.912515\nmax      64.000000    53.130000     5.000000  63770.428010","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>bmi</th>\n      <th>children</th>\n      <th>charges</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>1338.000000</td>\n      <td>1338.000000</td>\n      <td>1338.000000</td>\n      <td>1338.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>39.207025</td>\n      <td>30.663397</td>\n      <td>1.094918</td>\n      <td>13270.422265</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>14.049960</td>\n      <td>6.098187</td>\n      <td>1.205493</td>\n      <td>12110.011237</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>18.000000</td>\n      <td>15.960000</td>\n      <td>0.000000</td>\n      <td>1121.873900</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>27.000000</td>\n      <td>26.296250</td>\n      <td>0.000000</td>\n      <td>4740.287150</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>39.000000</td>\n      <td>30.400000</td>\n      <td>1.000000</td>\n      <td>9382.033000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>51.000000</td>\n      <td>34.693750</td>\n      <td>2.000000</td>\n      <td>16639.912515</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>64.000000</td>\n      <td>53.130000</td>\n      <td>5.000000</td>\n      <td>63770.428010</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"O dataset contém as seguintes colunas:\n\n`age`: idade do beneficiário principal\n\n`sex`: gênero do contratante do seguro\n\n`bmi`: índice de massa corporal do contratante\n\n`children`: número de dependentes\n\n`smoker`: se o contratante é fumante ou não\n\n`region`: região de residência do contratante\n\n`charges`: valor cobrado pelo seguro saúde\n\nAs colunas sex, smoker, e region são categóricas e precisarão ser convertidas em numéricas. Além disso, para o treinamento do modelo de rede neural, pode ser benéfico normalizar ou padronizar as características numéricas.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\n#Lista de colunas que contem dados categoricos e numericos\ncategorical_features = ['sex', 'smoker', 'region']\nnumeric_features = ['age', 'bmi', 'children']\n\nnumeric_transformer = StandardScaler()\ncategorical_transformer = OneHotEncoder(drop='first')\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)\n    ])\n\nX = df.drop('charges', axis=1)\ny = df['charges']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nX_train_preprocessed = preprocessor.fit_transform(X_train)\nX_test_preprocessed = preprocessor.transform(X_test)\n\nX_train_preprocessed_df = pd.DataFrame(X_train_preprocessed, columns=(numeric_features + \n                                                                      list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features))))\n\nX_train_preprocessed_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-07T20:38:12.829988Z","iopub.execute_input":"2023-11-07T20:38:12.831576Z","iopub.status.idle":"2023-11-07T20:38:14.527993Z","shell.execute_reply.started":"2023-11-07T20:38:12.831470Z","shell.execute_reply":"2023-11-07T20:38:14.526986Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"        age       bmi  children  sex_male  smoker_yes  region_northwest  \\\n0  0.472227 -1.756525  0.734336       0.0         0.0               1.0   \n1  0.543313 -1.033082 -0.911192       0.0         0.0               0.0   \n2  0.898745 -0.943687 -0.911192       0.0         0.0               0.0   \n3 -0.025379  0.622393  3.202629       0.0         0.0               0.0   \n4  1.040918 -1.504893  1.557100       0.0         0.0               1.0   \n\n   region_southeast  region_southwest  \n0               0.0               0.0  \n1               0.0               0.0  \n2               1.0               0.0  \n3               1.0               0.0  \n4               0.0               0.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>bmi</th>\n      <th>children</th>\n      <th>sex_male</th>\n      <th>smoker_yes</th>\n      <th>region_northwest</th>\n      <th>region_southeast</th>\n      <th>region_southwest</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.472227</td>\n      <td>-1.756525</td>\n      <td>0.734336</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.543313</td>\n      <td>-1.033082</td>\n      <td>-0.911192</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.898745</td>\n      <td>-0.943687</td>\n      <td>-0.911192</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.025379</td>\n      <td>0.622393</td>\n      <td>3.202629</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.040918</td>\n      <td>-1.504893</td>\n      <td>1.557100</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"#### Importação de Módulos\n- `StandardScaler`: Normaliza os dados numéricos para que tenham média 0 e desvio padrão 1. Algumas marcas de roupas podem usar números (como 40, 42, 44) e outras podem usar letras (P, M, G). O StandardScaler transforma todos os tamanhos em um padrão único para que a comparação seja justa. Para os dados, isso significa subtrair a média e dividir pelo desvio padrão para que a média dos dados transformados seja 0 e o desvio padrão seja 1.\n- `OneHotEncoder`: Converte variáveis categóricas em uma forma numérica adequada para modelos de machine learning. Suponha que você tem um grupo de animais incluindo gatos, cachorros e pássaros. O OneHotEncoder cria uma coluna separada para cada tipo de animal e marca com um \"1\" a coluna que corresponde ao animal presente. Então, se você tem um gato, a coluna 'gato' teria um \"1\" e as colunas 'cachorro' e 'pássaro' teriam um \"0\".\n- `ColumnTransformer`:  Permite que você aplique transformações diferentes (como normalização ou codificação one-hot) a diferentes colunas do seu conjunto de dados. É como se você pudesse dar instruções específicas para lidar com diferentes tipos de informações em um formulário.\n- `Pipeline`: ajuda a conectar várias etapas de processamento e modelagem em uma linha de produção, cada etapa é realizada em sequência.\n\n#### Transformadores\n- `numeric_transformer`: Configurado para normalizar os dados numéricos.\n- `categorical_transformer`: Configurado para transformar os dados categóricos em números usando one-hot encoding, evitando a multicolinearidade ao descartar uma das colunas resultantes. (Ali eu tenho 4 regiões, se uma das linhas não está em nenhuma de três regiões, ela com certeza está na quarta, de modo que a informação de uma das colunas de região é dispensável)\n\n#### Pré-Processamento\n- `preprocessor`: Aplica as transformações definidas acima às colunas correspondentes.\n\n#### Divisão dos Dados\n- `X` e `y`: As características e o alvo do nosso modelo, respectivamente.\n- `X_train, X_test, y_train, y_test`: Conjuntos de dados de treino e teste.\n\n#### Aplicação do Pré-Processamento\n- `fit_transform`: O pré-processador aprende e transforma os dados de treino.\n- `transform`: O pré-processador aplica a transformação aos dados de teste.\n\n#### Conversão para DataFrame\n- Os dados transformados são convertidos de volta em um formato de tabela para fácil análise.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.neural_network import MLPRegressor\nfrom scipy.stats import randint\nimport numpy as np\n\nhidden_layer_sizes = [tuple(randint.rvs(1, 201) for _ in range(num_layers)) for num_layers in randint.rvs(1, 5, size=10)]\n\nparam_distribs = {\n    'hidden_layer_sizes': hidden_layer_sizes,\n    'activation': ['tanh', 'relu'],\n    'solver': ['sgd', 'adam'],\n    'alpha': [0.0001, 0.001, 0.01, 0.1],\n    'learning_rate_init': [0.001, 0.01, 0.1],\n}\n\nmlp = MLPRegressor(max_iter=1000, random_state=42)\n\nrandom_search = RandomizedSearchCV(\n    estimator=mlp,\n    param_distributions=param_distribs,\n    n_iter=100,\n    scoring='neg_mean_squared_error',\n    n_jobs=-1,\n    cv=5,\n    random_state=42,\n    error_score=np.nan\n)\n\nrandom_search.fit(X_train_preprocessed, y_train)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_search.best_params_","metadata":{"execution":{"iopub.status.busy":"2023-11-07T21:40:11.572345Z","iopub.execute_input":"2023-11-07T21:40:11.572721Z","iopub.status.idle":"2023-11-07T21:40:11.582720Z","shell.execute_reply.started":"2023-11-07T21:40:11.572691Z","shell.execute_reply":"2023-11-07T21:40:11.581533Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"{'solver': 'adam',\n 'learning_rate_init': 0.001,\n 'hidden_layer_sizes': (189, 199, 23),\n 'alpha': 0.1,\n 'activation': 'relu'}"},"metadata":{}}]},{"cell_type":"code","source":"mlp_regressor = mlp_regressor = MLPRegressor(\n                    solver='adam',\n                    learning_rate_init=0.001,\n                    hidden_layer_sizes=(189, 199, 23),\n                    alpha=0.1,\n                    activation='relu',\n                    batch_size='auto', \n                    learning_rate='adaptive',\n                    max_iter=1000, \n                    tol=1e-5,\n                    random_state=42,\n                    verbose=True\n                )\n\npipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                           ('regressor', mlp_regressor)])\n\npipeline.fit(X_train, y_train)\n\ny_pred = pipeline.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-11-07T21:42:37.473130Z","iopub.execute_input":"2023-11-07T21:42:37.473464Z","iopub.status.idle":"2023-11-07T21:42:57.915495Z","shell.execute_reply.started":"2023-11-07T21:42:37.473439Z","shell.execute_reply":"2023-11-07T21:42:57.914688Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Iteration 1, loss = 161219960.17914197\nIteration 2, loss = 161205697.80366290\nIteration 3, loss = 161182699.19484201\nIteration 4, loss = 161143614.18094781\nIteration 5, loss = 161078170.31512612\nIteration 6, loss = 160973036.70195267\nIteration 7, loss = 160810453.51273832\nIteration 8, loss = 160566378.51449054\nIteration 9, loss = 160207356.79557341\nIteration 10, loss = 159694515.10719034\nIteration 11, loss = 159001296.68360114\nIteration 12, loss = 158020300.14189988\nIteration 13, loss = 156731280.58207455\nIteration 14, loss = 155061132.49711609\nIteration 15, loss = 152896894.85693228\nIteration 16, loss = 150170001.15788347\nIteration 17, loss = 146825967.85626173\nIteration 18, loss = 142870364.35962200\nIteration 19, loss = 138001266.15274402\nIteration 20, loss = 132487064.52870874\nIteration 21, loss = 126163584.86927146\nIteration 22, loss = 118972352.79527062\nIteration 23, loss = 111025760.00207788\nIteration 24, loss = 102633242.06909257\nIteration 25, loss = 93978128.33494985\nIteration 26, loss = 85266539.21320994\nIteration 27, loss = 77076809.78363553\nIteration 28, loss = 69496168.48617266\nIteration 29, loss = 63395496.01296513\nIteration 30, loss = 58312552.36723594\nIteration 31, loss = 54833680.30219634\nIteration 32, loss = 52590492.83310888\nIteration 33, loss = 51199389.48385400\nIteration 34, loss = 50367380.17267702\nIteration 35, loss = 49614297.39609531\nIteration 36, loss = 49039550.10298871\nIteration 37, loss = 48379566.58685927\nIteration 38, loss = 47711247.71786026\nIteration 39, loss = 47032920.41943969\nIteration 40, loss = 46431263.29758767\nIteration 41, loss = 45809865.41505445\nIteration 42, loss = 45195028.99231147\nIteration 43, loss = 44595455.28038651\nIteration 44, loss = 43986932.21562931\nIteration 45, loss = 43388846.37074587\nIteration 46, loss = 42792207.36769813\nIteration 47, loss = 42157668.13370045\nIteration 48, loss = 41590547.44078311\nIteration 49, loss = 40934857.91664444\nIteration 50, loss = 40341786.53054742\nIteration 51, loss = 39741244.42627098\nIteration 52, loss = 39138225.49722032\nIteration 53, loss = 38516467.50874138\nIteration 54, loss = 37894774.79544596\nIteration 55, loss = 37268251.59230026\nIteration 56, loss = 36627758.00836145\nIteration 57, loss = 35991466.15694716\nIteration 58, loss = 35387745.68974274\nIteration 59, loss = 34712711.31491262\nIteration 60, loss = 34065531.34782112\nIteration 61, loss = 33419170.10986249\nIteration 62, loss = 32771409.41042202\nIteration 63, loss = 32146939.07505990\nIteration 64, loss = 31488249.32708462\nIteration 65, loss = 30853906.93094838\nIteration 66, loss = 30226568.92795551\nIteration 67, loss = 29616046.22142408\nIteration 68, loss = 28983171.48537897\nIteration 69, loss = 28376400.84456862\nIteration 70, loss = 27742893.01825429\nIteration 71, loss = 27151366.85325551\nIteration 72, loss = 26541662.96807466\nIteration 73, loss = 25973157.99365136\nIteration 74, loss = 25387433.45582802\nIteration 75, loss = 24835061.44348814\nIteration 76, loss = 24280954.70156309\nIteration 77, loss = 23789412.76985161\nIteration 78, loss = 23270490.51207820\nIteration 79, loss = 22757992.27004515\nIteration 80, loss = 22309280.51354020\nIteration 81, loss = 21890721.54262596\nIteration 82, loss = 21452970.32562639\nIteration 83, loss = 21081712.28829082\nIteration 84, loss = 20710962.88788632\nIteration 85, loss = 20377770.18557331\nIteration 86, loss = 20053488.66363630\nIteration 87, loss = 19756630.60701468\nIteration 88, loss = 19506670.97874447\nIteration 89, loss = 19250725.62489783\nIteration 90, loss = 19019790.35061157\nIteration 91, loss = 18797545.06610442\nIteration 92, loss = 18613610.17785659\nIteration 93, loss = 18444727.24803863\nIteration 94, loss = 18277859.77718448\nIteration 95, loss = 18133643.47712396\nIteration 96, loss = 18004982.02733183\nIteration 97, loss = 17878582.81664762\nIteration 98, loss = 17755911.28786042\nIteration 99, loss = 17676992.12103684\nIteration 100, loss = 17585409.12277917\nIteration 101, loss = 17491549.14630668\nIteration 102, loss = 17420440.18383174\nIteration 103, loss = 17352650.56898191\nIteration 104, loss = 17281841.60512148\nIteration 105, loss = 17219677.80488423\nIteration 106, loss = 17156848.67693751\nIteration 107, loss = 17097222.08571870\nIteration 108, loss = 17039634.35766807\nIteration 109, loss = 16985775.27165551\nIteration 110, loss = 16931610.08434258\nIteration 111, loss = 16882992.53156489\nIteration 112, loss = 16835702.32997481\nIteration 113, loss = 16781308.92922132\nIteration 114, loss = 16744752.37170970\nIteration 115, loss = 16683667.06883162\nIteration 116, loss = 16641847.33050749\nIteration 117, loss = 16602696.34712597\nIteration 118, loss = 16565709.66053148\nIteration 119, loss = 16511692.78967511\nIteration 120, loss = 16483118.10973062\nIteration 121, loss = 16423257.01029621\nIteration 122, loss = 16378478.38306626\nIteration 123, loss = 16338676.52825129\nIteration 124, loss = 16303963.63345917\nIteration 125, loss = 16263786.26635037\nIteration 126, loss = 16221866.72136114\nIteration 127, loss = 16186530.79753581\nIteration 128, loss = 16148703.44477636\nIteration 129, loss = 16105412.97446579\nIteration 130, loss = 16070469.91933385\nIteration 131, loss = 16034981.28962739\nIteration 132, loss = 16004785.58817873\nIteration 133, loss = 15963389.61436242\nIteration 134, loss = 15927069.32869917\nIteration 135, loss = 15904091.55460908\nIteration 136, loss = 15861910.55380822\nIteration 137, loss = 15832864.14798762\nIteration 138, loss = 15804779.27555615\nIteration 139, loss = 15772021.07008843\nIteration 140, loss = 15743186.69003716\nIteration 141, loss = 15703256.24890766\nIteration 142, loss = 15678073.37181422\nIteration 143, loss = 15654594.40599932\nIteration 144, loss = 15619432.48963747\nIteration 145, loss = 15587064.03896404\nIteration 146, loss = 15550984.90737004\nIteration 147, loss = 15521563.49611470\nIteration 148, loss = 15491261.38579326\nIteration 149, loss = 15457337.80203355\nIteration 150, loss = 15434140.03471828\nIteration 151, loss = 15408702.48391063\nIteration 152, loss = 15371568.16026491\nIteration 153, loss = 15348324.28452844\nIteration 154, loss = 15324194.69169678\nIteration 155, loss = 15292044.95998175\nIteration 156, loss = 15266101.16409563\nIteration 157, loss = 15249383.13284935\nIteration 158, loss = 15214909.04887732\nIteration 159, loss = 15182142.93299726\nIteration 160, loss = 15153589.83733674\nIteration 161, loss = 15130140.18734708\nIteration 162, loss = 15095545.86757107\nIteration 163, loss = 15079688.49064885\nIteration 164, loss = 15061536.52608170\nIteration 165, loss = 15050869.46962580\nIteration 166, loss = 15002748.00504789\nIteration 167, loss = 14975770.92505723\nIteration 168, loss = 14949729.69032911\nIteration 169, loss = 14926438.15191603\nIteration 170, loss = 14894111.78461631\nIteration 171, loss = 14872146.40790615\nIteration 172, loss = 14851598.81469042\nIteration 173, loss = 14837320.32536745\nIteration 174, loss = 14792690.84834695\nIteration 175, loss = 14770839.94463437\nIteration 176, loss = 14749957.49095486\nIteration 177, loss = 14715345.11199003\nIteration 178, loss = 14688384.71110700\nIteration 179, loss = 14660717.81405561\nIteration 180, loss = 14642369.89884306\nIteration 181, loss = 14614567.37749559\nIteration 182, loss = 14584929.98630225\nIteration 183, loss = 14568974.14672589\nIteration 184, loss = 14543362.83166677\nIteration 185, loss = 14524801.89153099\nIteration 186, loss = 14485845.31454168\nIteration 187, loss = 14455805.17544250\nIteration 188, loss = 14437818.17616748\nIteration 189, loss = 14411756.24040330\nIteration 190, loss = 14379933.80660927\nIteration 191, loss = 14356766.75743024\nIteration 192, loss = 14327119.84551890\nIteration 193, loss = 14305676.35151775\nIteration 194, loss = 14278917.86517079\nIteration 195, loss = 14255130.73411888\nIteration 196, loss = 14220980.30698024\nIteration 197, loss = 14204394.79540095\nIteration 198, loss = 14181585.61299344\nIteration 199, loss = 14157246.21235420\nIteration 200, loss = 14127101.62478073\nIteration 201, loss = 14109888.60775533\nIteration 202, loss = 14088003.21574515\nIteration 203, loss = 14057470.42005974\nIteration 204, loss = 14036754.59064632\nIteration 205, loss = 14010690.27591983\nIteration 206, loss = 13991126.97017379\nIteration 207, loss = 13963817.97353597\nIteration 208, loss = 13945133.58755077\nIteration 209, loss = 13915977.05698931\nIteration 210, loss = 13887787.62998578\nIteration 211, loss = 13865430.49694178\nIteration 212, loss = 13842193.45683923\nIteration 213, loss = 13825003.15938641\nIteration 214, loss = 13796674.35878812\nIteration 215, loss = 13769952.20665856\nIteration 216, loss = 13746066.57806192\nIteration 217, loss = 13724555.37521083\nIteration 218, loss = 13698158.07600676\nIteration 219, loss = 13678516.99540820\nIteration 220, loss = 13651475.10685911\nIteration 221, loss = 13625124.93471602\nIteration 222, loss = 13608803.45591040\nIteration 223, loss = 13584854.42389039\nIteration 224, loss = 13551927.46220090\nIteration 225, loss = 13540551.43040418\nIteration 226, loss = 13511612.95878444\nIteration 227, loss = 13489550.30319191\nIteration 228, loss = 13464520.74187741\nIteration 229, loss = 13435288.86546450\nIteration 230, loss = 13421345.60321598\nIteration 231, loss = 13395856.51226345\nIteration 232, loss = 13369866.59506393\nIteration 233, loss = 13353675.64410012\nIteration 234, loss = 13323289.89794159\nIteration 235, loss = 13304809.14216192\nIteration 236, loss = 13280011.48886132\nIteration 237, loss = 13256228.90781333\nIteration 238, loss = 13232440.91871714\nIteration 239, loss = 13217555.65777015\nIteration 240, loss = 13188818.50823896\nIteration 241, loss = 13171138.29099719\nIteration 242, loss = 13141245.64016949\nIteration 243, loss = 13111731.65098341\nIteration 244, loss = 13090151.24348261\nIteration 245, loss = 13068416.92374479\nIteration 246, loss = 13047692.22400457\nIteration 247, loss = 13026168.46355235\nIteration 248, loss = 12996277.41299680\nIteration 249, loss = 12975090.02931091\nIteration 250, loss = 12954327.18996853\nIteration 251, loss = 12926675.69731933\nIteration 252, loss = 12905564.83720246\nIteration 253, loss = 12887134.17954606\nIteration 254, loss = 12861792.04827110\nIteration 255, loss = 12842098.78395587\nIteration 256, loss = 12821262.97394601\nIteration 257, loss = 12796507.49684020\nIteration 258, loss = 12771590.89592718\nIteration 259, loss = 12751313.93115631\nIteration 260, loss = 12727952.91732365\nIteration 261, loss = 12707928.55708564\nIteration 262, loss = 12700360.41944980\nIteration 263, loss = 12670183.45701542\nIteration 264, loss = 12644263.00576726\nIteration 265, loss = 12621710.46771224\nIteration 266, loss = 12598281.04374510\nIteration 267, loss = 12579739.10672226\nIteration 268, loss = 12557418.23710368\nIteration 269, loss = 12534794.01668394\nIteration 270, loss = 12527697.33420998\nIteration 271, loss = 12490885.48265982\nIteration 272, loss = 12466253.62222484\nIteration 273, loss = 12449838.97857550\nIteration 274, loss = 12434470.93257159\nIteration 275, loss = 12406007.04452504\nIteration 276, loss = 12380558.24560533\nIteration 277, loss = 12362703.39204674\nIteration 278, loss = 12344219.31161044\nIteration 279, loss = 12324203.99639880\nIteration 280, loss = 12285629.97829335\nIteration 281, loss = 12269008.69288455\nIteration 282, loss = 12244611.49752986\nIteration 283, loss = 12228674.23520932\nIteration 284, loss = 12199902.05882176\nIteration 285, loss = 12180038.72075303\nIteration 286, loss = 12168841.59440752\nIteration 287, loss = 12142732.51243999\nIteration 288, loss = 12117810.58519892\nIteration 289, loss = 12098840.65787459\nIteration 290, loss = 12071290.67553944\nIteration 291, loss = 12058965.46217132\nIteration 292, loss = 12031073.02075840\nIteration 293, loss = 12015516.44871291\nIteration 294, loss = 11990849.38294915\nIteration 295, loss = 11968338.00904918\nIteration 296, loss = 11947806.35416351\nIteration 297, loss = 11931282.62306995\nIteration 298, loss = 11920457.74995221\nIteration 299, loss = 11884086.29560149\nIteration 300, loss = 11876648.99436510\nIteration 301, loss = 11853367.59367857\nIteration 302, loss = 11836382.84223356\nIteration 303, loss = 11815040.59478405\nIteration 304, loss = 11802182.83964005\nIteration 305, loss = 11773377.53783254\nIteration 306, loss = 11759262.52197531\nIteration 307, loss = 11740797.32691217\nIteration 308, loss = 11725065.92421136\nIteration 309, loss = 11706273.45579616\nIteration 310, loss = 11686394.13931371\nIteration 311, loss = 11676882.13636395\nIteration 312, loss = 11658094.41895291\nIteration 313, loss = 11647736.66176811\nIteration 314, loss = 11626261.70440473\nIteration 315, loss = 11615080.17209182\nIteration 316, loss = 11597898.19053743\nIteration 317, loss = 11577588.02647885\nIteration 318, loss = 11561593.11996124\nIteration 319, loss = 11546155.05895515\nIteration 320, loss = 11535314.44068789\nIteration 321, loss = 11517157.67477773\nIteration 322, loss = 11503280.62318899\nIteration 323, loss = 11491233.66678188\nIteration 324, loss = 11472387.96920978\nIteration 325, loss = 11457289.58244910\nIteration 326, loss = 11448015.17822544\nIteration 327, loss = 11432426.09959865\nIteration 328, loss = 11420917.92582718\nIteration 329, loss = 11406374.66624457\nIteration 330, loss = 11389946.30682342\nIteration 331, loss = 11385291.56553952\nIteration 332, loss = 11373550.64679269\nIteration 333, loss = 11357151.08700957\nIteration 334, loss = 11364963.93486443\nIteration 335, loss = 11327998.90075873\nIteration 336, loss = 11331295.40307936\nIteration 337, loss = 11317361.18408630\nIteration 338, loss = 11294621.04362911\nIteration 339, loss = 11279650.34403011\nIteration 340, loss = 11266337.86140647\nIteration 341, loss = 11274201.24540767\nIteration 342, loss = 11245433.52222783\nIteration 343, loss = 11235150.29792589\nIteration 344, loss = 11232083.21442833\nIteration 345, loss = 11215290.46605799\nIteration 346, loss = 11202891.02377280\nIteration 347, loss = 11193000.50931713\nIteration 348, loss = 11187862.63953776\nIteration 349, loss = 11187370.13863285\nIteration 350, loss = 11166241.06923166\nIteration 351, loss = 11171574.42380829\nIteration 352, loss = 11153009.18741464\nIteration 353, loss = 11132106.53869613\nIteration 354, loss = 11133751.15936537\nIteration 355, loss = 11132155.27991441\nIteration 356, loss = 11113655.10158665\nIteration 357, loss = 11105764.61226201\nIteration 358, loss = 11094494.60941592\nIteration 359, loss = 11086388.29960869\nIteration 360, loss = 11077681.04722582\nIteration 361, loss = 11068709.12048082\nIteration 362, loss = 11059135.50379835\nIteration 363, loss = 11058269.06792887\nIteration 364, loss = 11047012.17936451\nIteration 365, loss = 11034018.48416527\nIteration 366, loss = 11032585.24401008\nIteration 367, loss = 11016899.10383181\nIteration 368, loss = 11009715.32477520\nIteration 369, loss = 11002155.28178303\nIteration 370, loss = 10992190.42050787\nIteration 371, loss = 10989494.96304010\nIteration 372, loss = 10993137.51920809\nIteration 373, loss = 10985767.86882010\nIteration 374, loss = 10967428.86071356\nIteration 375, loss = 10955412.23626303\nIteration 376, loss = 10947725.52140716\nIteration 377, loss = 10947059.70699753\nIteration 378, loss = 10936466.99764193\nIteration 379, loss = 10933062.86961123\nIteration 380, loss = 10927717.49542762\nIteration 381, loss = 10922988.88038077\nIteration 382, loss = 10911725.35037627\nIteration 383, loss = 10903244.88297827\nIteration 384, loss = 10894182.50834232\nIteration 385, loss = 10883774.78746049\nIteration 386, loss = 10894008.98553494\nIteration 387, loss = 10889217.50679248\nIteration 388, loss = 10870780.24166688\nIteration 389, loss = 10861585.21884860\nIteration 390, loss = 10853414.51613285\nIteration 391, loss = 10843710.83789855\nIteration 392, loss = 10841540.46034503\nIteration 393, loss = 10844276.13503658\nIteration 394, loss = 10822152.89295473\nIteration 395, loss = 10823848.70582277\nIteration 396, loss = 10831876.10434623\nIteration 397, loss = 10814053.02766171\nIteration 398, loss = 10809374.67026408\nIteration 399, loss = 10799052.13217900\nIteration 400, loss = 10795760.58023443\nIteration 401, loss = 10794814.48494401\nIteration 402, loss = 10776669.49192672\nIteration 403, loss = 10777721.65884521\nIteration 404, loss = 10774243.77388146\nIteration 405, loss = 10765217.54157869\nIteration 406, loss = 10757801.15182370\nIteration 407, loss = 10760986.63311758\nIteration 408, loss = 10746549.17558957\nIteration 409, loss = 10739548.34316598\nIteration 410, loss = 10769873.78169007\nIteration 411, loss = 10762794.01468608\nIteration 412, loss = 10731207.94109174\nIteration 413, loss = 10724646.63221763\nIteration 414, loss = 10714699.38803548\nIteration 415, loss = 10720374.46264012\nIteration 416, loss = 10710856.90710236\nIteration 417, loss = 10698429.60483761\nIteration 418, loss = 10720606.07036014\nIteration 419, loss = 10696495.37145109\nIteration 420, loss = 10685541.48552402\nIteration 421, loss = 10681655.49049841\nIteration 422, loss = 10667067.28963210\nIteration 423, loss = 10659787.36130721\nIteration 424, loss = 10659799.68457095\nIteration 425, loss = 10653389.50706806\nIteration 426, loss = 10646361.42325282\nIteration 427, loss = 10648984.82964819\nIteration 428, loss = 10640536.87813549\nIteration 429, loss = 10634585.84424537\nIteration 430, loss = 10629936.28840973\nIteration 431, loss = 10618734.23030445\nIteration 432, loss = 10625215.57196182\nIteration 433, loss = 10619593.10515379\nIteration 434, loss = 10635306.90639657\nIteration 435, loss = 10604200.30480862\nIteration 436, loss = 10602946.31256834\nIteration 437, loss = 10597300.18048002\nIteration 438, loss = 10575899.02596434\nIteration 439, loss = 10570906.36202388\nIteration 440, loss = 10566836.27555341\nIteration 441, loss = 10558434.06903771\nIteration 442, loss = 10562793.06575046\nIteration 443, loss = 10547105.96698423\nIteration 444, loss = 10541106.47259229\nIteration 445, loss = 10549581.73993844\nIteration 446, loss = 10538905.01678412\nIteration 447, loss = 10541502.13376690\nIteration 448, loss = 10527005.04110619\nIteration 449, loss = 10528496.92321653\nIteration 450, loss = 10511809.26829148\nIteration 451, loss = 10520209.99999727\nIteration 452, loss = 10512188.40923602\nIteration 453, loss = 10508519.32362637\nIteration 454, loss = 10499333.24386865\nIteration 455, loss = 10491312.99527267\nIteration 456, loss = 10485185.76695739\nIteration 457, loss = 10480885.12508875\nIteration 458, loss = 10475765.11258653\nIteration 459, loss = 10475137.86320487\nIteration 460, loss = 10475706.04398012\nIteration 461, loss = 10459738.76304326\nIteration 462, loss = 10486013.20551687\nIteration 463, loss = 10463291.78411810\nIteration 464, loss = 10457409.62460759\nIteration 465, loss = 10450284.10526951\nIteration 466, loss = 10440490.30599273\nIteration 467, loss = 10437874.61803942\nIteration 468, loss = 10429502.12961141\nIteration 469, loss = 10435389.37680434\nIteration 470, loss = 10434167.57274592\nIteration 471, loss = 10424075.47975557\nIteration 472, loss = 10413492.16122875\nIteration 473, loss = 10408381.97625198\nIteration 474, loss = 10405773.81573820\nIteration 475, loss = 10403215.31588724\nIteration 476, loss = 10400094.62990683\nIteration 477, loss = 10390706.99573209\nIteration 478, loss = 10391194.45179226\nIteration 479, loss = 10390804.17396851\nIteration 480, loss = 10397873.09014650\nIteration 481, loss = 10369090.01416193\nIteration 482, loss = 10386085.27789303\nIteration 483, loss = 10363575.48449039\nIteration 484, loss = 10379682.20235865\nIteration 485, loss = 10376894.96476618\nIteration 486, loss = 10361649.21047161\nIteration 487, loss = 10347513.62039679\nIteration 488, loss = 10346369.21321141\nIteration 489, loss = 10344060.27365335\nIteration 490, loss = 10341055.54828484\nIteration 491, loss = 10332501.19900293\nIteration 492, loss = 10329408.28011767\nIteration 493, loss = 10331809.17086819\nIteration 494, loss = 10327282.91616030\nIteration 495, loss = 10320453.46760630\nIteration 496, loss = 10316960.30208314\nIteration 497, loss = 10307998.70511238\nIteration 498, loss = 10306278.96552144\nIteration 499, loss = 10307922.00305348\nIteration 500, loss = 10298231.29517218\nIteration 501, loss = 10293122.52186383\nIteration 502, loss = 10300992.37251434\nIteration 503, loss = 10293913.80947832\nIteration 504, loss = 10282155.68402764\nIteration 505, loss = 10294993.46711909\nIteration 506, loss = 10276769.82470779\nIteration 507, loss = 10277430.71428629\nIteration 508, loss = 10272596.08830304\nIteration 509, loss = 10266681.14368620\nIteration 510, loss = 10261969.50538677\nIteration 511, loss = 10253372.36679051\nIteration 512, loss = 10254229.09774355\nIteration 513, loss = 10254264.42848635\nIteration 514, loss = 10245125.33922001\nIteration 515, loss = 10238311.79860648\nIteration 516, loss = 10237352.62087931\nIteration 517, loss = 10233323.01459888\nIteration 518, loss = 10225944.78858840\nIteration 519, loss = 10222704.99499380\nIteration 520, loss = 10221483.76507307\nIteration 521, loss = 10219850.86159921\nIteration 522, loss = 10218479.19471580\nIteration 523, loss = 10212203.97670405\nIteration 524, loss = 10203310.65787717\nIteration 525, loss = 10209433.74531994\nIteration 526, loss = 10201099.65453546\nIteration 527, loss = 10198494.07568562\nIteration 528, loss = 10200032.64630930\nIteration 529, loss = 10189257.07465232\nIteration 530, loss = 10197758.72863145\nIteration 531, loss = 10186314.07176206\nIteration 532, loss = 10174875.79919307\nIteration 533, loss = 10174352.37062330\nIteration 534, loss = 10171226.67228750\nIteration 535, loss = 10168484.78784252\nIteration 536, loss = 10175474.71796058\nIteration 537, loss = 10175372.09789944\nIteration 538, loss = 10154129.11943499\nIteration 539, loss = 10179969.85258313\nIteration 540, loss = 10163680.85568026\nIteration 541, loss = 10147973.76269969\nIteration 542, loss = 10152008.07464217\nIteration 543, loss = 10144163.14062440\nIteration 544, loss = 10139324.70315622\nIteration 545, loss = 10144815.08453708\nIteration 546, loss = 10138262.94200604\nIteration 547, loss = 10140810.91214180\nIteration 548, loss = 10137658.60202411\nIteration 549, loss = 10125694.72497917\nIteration 550, loss = 10112213.98551209\nIteration 551, loss = 10123775.17139149\nIteration 552, loss = 10112127.23679474\nIteration 553, loss = 10140240.12959217\nIteration 554, loss = 10114191.55912328\nIteration 555, loss = 10106666.98630534\nIteration 556, loss = 10106002.27099228\nIteration 557, loss = 10094279.29689666\nIteration 558, loss = 10112452.79306227\nIteration 559, loss = 10104208.79435094\nIteration 560, loss = 10097745.04158202\nIteration 561, loss = 10086183.19205701\nIteration 562, loss = 10078755.63747244\nIteration 563, loss = 10076983.82484161\nIteration 564, loss = 10094064.86508590\nIteration 565, loss = 10070806.91853032\nIteration 566, loss = 10074565.38997988\nIteration 567, loss = 10075753.63867306\nIteration 568, loss = 10061696.94754003\nIteration 569, loss = 10097205.66443905\nIteration 570, loss = 10079692.51697934\nIteration 571, loss = 10063200.95991873\nIteration 572, loss = 10069822.06761486\nIteration 573, loss = 10049019.02697149\nIteration 574, loss = 10058373.24378271\nIteration 575, loss = 10042212.26671296\nIteration 576, loss = 10046795.23633792\nIteration 577, loss = 10042919.64904262\nIteration 578, loss = 10036872.07880329\nIteration 579, loss = 10036959.71088646\nIteration 580, loss = 10045434.12357776\nIteration 581, loss = 10030173.80536791\nIteration 582, loss = 10030439.47006063\nIteration 583, loss = 10033515.74527160\nIteration 584, loss = 10031802.68879086\nIteration 585, loss = 10004134.67913214\nIteration 586, loss = 10019976.58921313\nIteration 587, loss = 10011069.10838853\nIteration 588, loss = 10017661.58865654\nIteration 589, loss = 10005452.48513654\nIteration 590, loss = 10005248.26098624\nIteration 591, loss = 10001760.17510503\nIteration 592, loss = 9990691.25810597\nIteration 593, loss = 9996956.80386337\nIteration 594, loss = 9996347.09254544\nIteration 595, loss = 9989342.54261549\nIteration 596, loss = 9994371.93402061\nIteration 597, loss = 10005305.49449935\nIteration 598, loss = 9983842.06223129\nIteration 599, loss = 9971366.98544806\nIteration 600, loss = 9973939.56394767\nIteration 601, loss = 9965036.08654522\nIteration 602, loss = 9969027.17535516\nIteration 603, loss = 9963496.12641462\nIteration 604, loss = 9971861.37963519\nIteration 605, loss = 9953812.88933139\nIteration 606, loss = 9955402.67622850\nIteration 607, loss = 9953610.93165642\nIteration 608, loss = 9958848.47426856\nIteration 609, loss = 9952511.96841072\nIteration 610, loss = 9955560.97795993\nIteration 611, loss = 9963718.80210703\nIteration 612, loss = 9965754.83885618\nIteration 613, loss = 9944179.04212478\nIteration 614, loss = 9934845.96934341\nIteration 615, loss = 9933168.14532924\nIteration 616, loss = 9943263.67513112\nIteration 617, loss = 9931361.98150912\nIteration 618, loss = 9931206.92108322\nIteration 619, loss = 9923582.00828737\nIteration 620, loss = 9922999.46282304\nIteration 621, loss = 9923179.51325833\nIteration 622, loss = 9911947.31333172\nIteration 623, loss = 9920548.31760078\nIteration 624, loss = 9908827.12110195\nIteration 625, loss = 9916117.29283884\nIteration 626, loss = 9945594.78332868\nIteration 627, loss = 9904116.70219222\nIteration 628, loss = 9922647.00972667\nIteration 629, loss = 9919531.87395673\nIteration 630, loss = 9919932.53930726\nIteration 631, loss = 9890909.03138256\nIteration 632, loss = 9904653.89606796\nIteration 633, loss = 9890942.05595919\nIteration 634, loss = 9890354.60285616\nIteration 635, loss = 9893842.41047007\nIteration 636, loss = 9878343.39746442\nIteration 637, loss = 9913194.76622018\nIteration 638, loss = 9894101.43968335\nIteration 639, loss = 9882548.23887787\nIteration 640, loss = 9877145.65257044\nIteration 641, loss = 9882672.05847334\nIteration 642, loss = 9921680.83471382\nIteration 643, loss = 9880000.01517160\nIteration 644, loss = 9878287.32092909\nIteration 645, loss = 9880627.54898182\nIteration 646, loss = 9862131.40364723\nIteration 647, loss = 9869257.17961868\nIteration 648, loss = 9865046.16839488\nIteration 649, loss = 9854878.82006932\nIteration 650, loss = 9856802.06525788\nIteration 651, loss = 9855420.71081656\nIteration 652, loss = 9854732.03959148\nIteration 653, loss = 9851463.61028341\nIteration 654, loss = 9856424.21162575\nIteration 655, loss = 9843489.56503032\nIteration 656, loss = 9854839.01569297\nIteration 657, loss = 9850992.80733263\nIteration 658, loss = 9846144.61584774\nIteration 659, loss = 9845854.66703764\nIteration 660, loss = 9850966.36412015\nIteration 661, loss = 9833127.77574422\nIteration 662, loss = 9825210.12214522\nIteration 663, loss = 9834730.94809720\nIteration 664, loss = 9830186.03269566\nIteration 665, loss = 9828299.24955428\nIteration 666, loss = 9833329.39299299\nIteration 667, loss = 9826962.28500098\nIteration 668, loss = 9821422.95370103\nIteration 669, loss = 9819853.75247208\nIteration 670, loss = 9815931.49375033\nIteration 671, loss = 9820594.97136201\nIteration 672, loss = 9812405.33190492\nIteration 673, loss = 9814589.14071510\nIteration 674, loss = 9814655.00599036\nIteration 675, loss = 9799265.02066641\nIteration 676, loss = 9808593.49723830\nIteration 677, loss = 9813093.15043513\nIteration 678, loss = 9812614.84409215\nIteration 679, loss = 9800469.03746137\nIteration 680, loss = 9816097.85385474\nIteration 681, loss = 9801910.97682744\nIteration 682, loss = 9801348.97938601\nIteration 683, loss = 9800677.97368941\nIteration 684, loss = 9808854.37072274\nIteration 685, loss = 9791219.39217773\nIteration 686, loss = 9781789.46902019\nIteration 687, loss = 9787535.88562513\nIteration 688, loss = 9782050.65564946\nIteration 689, loss = 9782819.87146233\nIteration 690, loss = 9781297.71199462\nIteration 691, loss = 9778238.34948747\nIteration 692, loss = 9772697.30394222\nIteration 693, loss = 9769638.08423709\nIteration 694, loss = 9767476.38075565\nIteration 695, loss = 9768143.82968261\nIteration 696, loss = 9764670.29067962\nIteration 697, loss = 9761693.26011162\nIteration 698, loss = 9781177.04008449\nIteration 699, loss = 9768127.20650417\nIteration 700, loss = 9761982.58211995\nIteration 701, loss = 9759233.15028945\nIteration 702, loss = 9755282.69008367\nIteration 703, loss = 9752268.24989569\nIteration 704, loss = 9746603.82440693\nIteration 705, loss = 9763675.73799388\nIteration 706, loss = 9748551.08953678\nIteration 707, loss = 9740489.20082248\nIteration 708, loss = 9734582.27365779\nIteration 709, loss = 9751605.84918860\nIteration 710, loss = 9749226.81044199\nIteration 711, loss = 9767891.02851086\nIteration 712, loss = 9745428.57670735\nIteration 713, loss = 9731641.71172976\nIteration 714, loss = 9742225.49223452\nIteration 715, loss = 9731804.88892470\nIteration 716, loss = 9729604.01957088\nIteration 717, loss = 9758304.74749801\nIteration 718, loss = 9731399.60468355\nIteration 719, loss = 9712512.21374737\nIteration 720, loss = 9737971.24777632\nIteration 721, loss = 9740380.32576780\nIteration 722, loss = 9732942.30085092\nIteration 723, loss = 9731365.42611548\nIteration 724, loss = 9714613.85569295\nIteration 725, loss = 9737238.88934556\nIteration 726, loss = 9709399.62983721\nIteration 727, loss = 9718494.07717328\nIteration 728, loss = 9715726.53684014\nIteration 729, loss = 9711081.86614347\nIteration 730, loss = 9706415.14344675\nIteration 731, loss = 9710153.88122622\nIteration 732, loss = 9701252.17457641\nIteration 733, loss = 9699069.49808871\nIteration 734, loss = 9702993.75103104\nIteration 735, loss = 9684107.49398105\nIteration 736, loss = 9697598.00293059\nIteration 737, loss = 9695145.31938406\nIteration 738, loss = 9691123.21794615\nIteration 739, loss = 9686606.38631438\nIteration 740, loss = 9693585.25331298\nIteration 741, loss = 9687467.01814656\nIteration 742, loss = 9687830.45568565\nIteration 743, loss = 9679990.36690090\nIteration 744, loss = 9683673.82499534\nIteration 745, loss = 9677435.65991606\nIteration 746, loss = 9673316.88214663\nIteration 747, loss = 9675835.87430988\nIteration 748, loss = 9677591.90981098\nIteration 749, loss = 9673901.26680317\nIteration 750, loss = 9681745.22077091\nIteration 751, loss = 9687204.33250050\nIteration 752, loss = 9679151.72455645\nIteration 753, loss = 9669070.59683662\nIteration 754, loss = 9669882.94216889\nIteration 755, loss = 9655368.22899373\nIteration 756, loss = 9661251.44089119\nIteration 757, loss = 9665246.84902905\nIteration 758, loss = 9656819.50900599\nIteration 759, loss = 9653982.28983735\nIteration 760, loss = 9654597.83910141\nIteration 761, loss = 9668923.14828760\nIteration 762, loss = 9664506.24198845\nIteration 763, loss = 9652493.94643088\nIteration 764, loss = 9658810.76089407\nIteration 765, loss = 9647911.48677739\nIteration 766, loss = 9651445.50729397\nIteration 767, loss = 9666736.67799808\nIteration 768, loss = 9637915.34439086\nIteration 769, loss = 9663034.34699533\nIteration 770, loss = 9645648.79564847\nIteration 771, loss = 9649897.19318085\nIteration 772, loss = 9646530.05066893\nIteration 773, loss = 9635805.21266988\nIteration 774, loss = 9632366.74441569\nIteration 775, loss = 9638031.70496693\nIteration 776, loss = 9636498.77460083\nIteration 777, loss = 9624445.90279049\nIteration 778, loss = 9662680.13016189\nIteration 779, loss = 9616381.86987323\nIteration 780, loss = 9621580.61227861\nIteration 781, loss = 9618094.58741614\nIteration 782, loss = 9618974.76648297\nIteration 783, loss = 9615379.86738293\nIteration 784, loss = 9617515.01592976\nIteration 785, loss = 9615241.37927081\nIteration 786, loss = 9612291.60975483\nIteration 787, loss = 9607098.16234520\nIteration 788, loss = 9609367.89094066\nIteration 789, loss = 9607472.31532211\nIteration 790, loss = 9611254.52409664\nIteration 791, loss = 9606593.84532183\nIteration 792, loss = 9622251.73715529\nIteration 793, loss = 9602192.38983450\nIteration 794, loss = 9612881.67706700\nIteration 795, loss = 9602105.12281407\nIteration 796, loss = 9592686.41655641\nIteration 797, loss = 9615136.13018373\nIteration 798, loss = 9598516.63050132\nIteration 799, loss = 9594537.91151625\nIteration 800, loss = 9581051.48651368\nIteration 801, loss = 9631770.26854511\nIteration 802, loss = 9607928.79870330\nIteration 803, loss = 9606853.30071156\nIteration 804, loss = 9591572.24691767\nIteration 805, loss = 9594655.99157216\nIteration 806, loss = 9583608.78670644\nIteration 807, loss = 9589928.98194866\nIteration 808, loss = 9597857.01286218\nIteration 809, loss = 9594926.02048548\nIteration 810, loss = 9585054.53538362\nIteration 811, loss = 9581084.75084976\nTraining loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error, r2_score\n\n# Erro medio absoluto\nmae = mean_absolute_error(y_test, y_pred)\n\n# Calcula MSE e RMSE\nmse = mean_squared_error(y_test, y_pred)\nrmse = mse**0.5\n\n# Calcula o R-quadrado (R²)\nr2 = r2_score(y_test, y_pred)\n\n# Exibe as métricas\nprint(\"MAE:\", mae)\nprint(\"MSE:\", mse)\nprint(\"RMSE:\", rmse)\nprint(\"R²:\", r2)","metadata":{"execution":{"iopub.status.busy":"2023-11-07T21:43:10.755949Z","iopub.execute_input":"2023-11-07T21:43:10.756747Z","iopub.status.idle":"2023-11-07T21:43:10.766078Z","shell.execute_reply.started":"2023-11-07T21:43:10.756713Z","shell.execute_reply":"2023-11-07T21:43:10.765105Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"MAE: 2425.825233512511\nMSE: 18621273.660115257\nRMSE: 4315.237381664566\nR²: 0.8800552282702421\n","output_type":"stream"}]},{"cell_type":"markdown","source":"- MAE (Erro Médio Absoluto): O valor de 2425.82 está em torno de 20% do valor médio dos custos de seguro (13.270,42). Isso significa que, em média, as previsões do modelo estão desviando 2425.82 do valor real. Embora não seja um erro pequeno, é importante lembrar que em problemas de regressão, sobretudo com uma grande variação nos valores da variável alvo (como é o caso com a variável charges), um certo nível de erro é esperado.\n\n- RMSE (Raiz do Erro Quadrático Médio): O valor de 4315.23 é aproximadamente 34% do valor médio dos custos de seguro. O RMSE dá uma ideia do erro padrão das previsões. Considerando que o desvio padrão dos custos de seguro é de 12.110,01, o RMSE é significativamente menor do que a variabilidade natural dos dados, o que é um sinal positivo.\n\n- R² (R-quadrado): Um valor de 0.88 é bastante alto e indica que o modelo está explicando uma grande parte da variância dos dados. No entanto, é importante lembrar que R² não informa sobre a precisão das previsões em um nível absoluto.\n\n#### Possíveis razões para o alto MAE e RMSE:\n- Escalabilidade dos Custos de Seguro: Dado que a faixa dos custos de seguro é bastante ampla (de cerca de 1.121,87 a 63.770,43), é razoável esperar que a previsão exata de todos os valores seja desafiadora.\n\n- Outliers: Se houver outliers nos custos de seguro, eles poderiam distorcer o MAE e o RMSE para cima, especialmente o RMSE, que é mais sensível a erros maiores devido ao quadrado dos erros.\n\n- Complexidade do Modelo: O modelo pode não ser suficientemente complexo para capturar todas as nuances dos dados (o que é verdade) ou pode ser que esteja faltando algum recurso informativo que não está sendo considerado.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pandas as pd\n\nerrors = y_test - y_pred\n\nplt.figure(figsize=(10,6))\nplt.hist(errors, bins=30, edgecolor='black')\nplt.title('Histograma dos Erros')\nplt.xlabel('Erro')\nplt.ylabel('Frequência')\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-07T22:16:54.619227Z","iopub.execute_input":"2023-11-07T22:16:54.619579Z","iopub.status.idle":"2023-11-07T22:16:54.937739Z","shell.execute_reply.started":"2023-11-07T22:16:54.619553Z","shell.execute_reply":"2023-11-07T22:16:54.935952Z"},"trusted":true},"execution_count":27,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x600 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAA1MAAAIjCAYAAADm7UHpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABHB0lEQVR4nO3deXhTZf7//1dKd6CUttCyFCiLrAoKgijiQqEIKiKjIKCAjOhHFBHUkVHZRFkckRFRdFTQcQA31BlHkAoILoiAiAKd4oKWrUXWAl1Im/v3hz/yJbRCe5PQJH0+rquX5Jw7d94n7+S0L3POicMYYwQAAAAAKJeQii4AAAAAAAIRYQoAAAAALBCmAAAAAMACYQoAAAAALBCmAAAAAMACYQoAAAAALBCmAAAAAMACYQoAAAAALBCmAAAAAMACYQoAglyjRo00dOjQii6jUuK5B4DgRpgCgAAyf/58ORwOrV+/vtT1V155pdq0aXPWj/PRRx9p4sSJZz0Pzq1ffvlFDofjD3+mTZtW0SUCQFAJregCAAC+lZmZqZCQ8v2/s48++khz5swhUAWoW265Rb169Sqx/MILL6yAagAgeBGmACDIRUREVHQJ5Xbs2DFVrVq1ossIWBdddJEGDx5crvsYY1RQUKCoqKgS6woKChQeHl7uUA4AwY69IgAEuVPP23E6nZo0aZKaNWumyMhIxcfHq0uXLkpPT5ckDR06VHPmzJEkj0PETjh27JjGjh2r5ORkRUREqHnz5vrb3/4mY4zH4+bn52vUqFFKSEhQ9erVdf3112vXrl1yOBwen3hNnDhRDodDW7du1cCBA1WzZk116dJFkvTdd99p6NChaty4sSIjI5WUlKTbb79d+/fv93isE3Ns27ZNgwcPVo0aNVSrVi099thjMsZox44d6tOnj2JiYpSUlKSnn37a4/7Hjx/X+PHj1b59e9WoUUNVq1bV5ZdfrpUrV5bpOTbGaMqUKapfv76io6N11VVXacuWLaWO/fnnn3XTTTcpLi5O0dHRuuSSS/Tf//63xLjZs2erdevWio6OVs2aNdWhQwctWLCgTPWURaNGjXTttdfq448/VocOHRQVFaUXX3xRn376qRwOhxYtWqRHH31U9erVU3R0tHJzcyVJb7/9ttq3b6+oqCglJCRo8ODB2rVrl8fc2dnZGjZsmOrXr6+IiAjVqVNHffr00S+//OK1+gHAH/DJFAAEoMOHD2vfvn0lljudzjPed+LEiZo6dar+/Oc/q2PHjsrNzdX69ev1zTffqHv37rrzzju1e/dupaen65///KfHfY0xuv7667Vy5UoNHz5c7dq108cff6wHH3xQu3bt0jPPPOMeO3ToUL311lu69dZbdckll2jVqlXq3bv3H9Z10003qVmzZnryySfdwSw9PV0///yzhg0bpqSkJG3ZskUvvfSStmzZoq+++soj5ElS//791bJlS02bNk3//e9/NWXKFMXFxenFF1/U1VdfrenTp+tf//qXHnjgAV188cXq2rWrJCk3N1cvv/yybrnlFt1xxx06cuSIXnnlFaWlpenrr79Wu3btTvucjh8/XlOmTFGvXr3Uq1cvffPNN+rRo4eOHz/uMS4nJ0eXXnqp8vLyNGrUKMXHx+u1117T9ddfr3feeUd9+/aVJP3jH//QqFGj9Kc//Un33XefCgoK9N1332nt2rUaOHDg6RssKS8vr9TXR2xsrEJD/9+v/szMTN1yyy268847dccdd6h58+budY8//rjCw8P1wAMPqLCwUOHh4Zo/f76GDRumiy++WFOnTlVOTo7+/ve/64svvtDGjRsVGxsrSerXr5+2bNmie++9V40aNdLevXuVnp6urKwsNWrU6Iz1A0DAMACAgDFv3jwj6bQ/rVu39rhPw4YNzZAhQ9y327Zta3r37n3axxk5cqQp7VfE+++/bySZKVOmeCz/05/+ZBwOh/nxxx+NMcZs2LDBSDKjR4/2GDd06FAjyUyYMMG9bMKECUaSueWWW0o8Xl5eXollCxcuNJLM6tWrS8wxYsQI97KioiJTv35943A4zLRp09zLDx48aKKiojyek6KiIlNYWOjxOAcPHjSJiYnm9ttvL1HDyfbu3WvCw8NN7969jcvlci//61//aiR5PM7o0aONJPPZZ5+5lx05csSkpKSYRo0ameLiYmOMMX369CnRx7LYvn37aV8ba9ascY9t2LChkWSWLl3qMcfKlSuNJNO4cWOP5//48eOmdu3apk2bNiY/P9+9/MMPPzSSzPjx440xvz9vksxTTz1V7voBINBwmB8ABKA5c+YoPT29xM8FF1xwxvvGxsZqy5Yt+uGHH8r9uB999JGqVKmiUaNGeSwfO3asjDFasmSJJGnp0qWSpLvvvttj3L333vuHc991110llp18/k5BQYH27dunSy65RJL0zTfflBj/5z//2f3vKlWqqEOHDjLGaPjw4e7lsbGxat68uX7++WePseHh4ZIkl8ulAwcOqKioSB06dCj1cU72ySef6Pjx47r33ns9PikbPXp0ibEfffSROnbs6D6MUZKqVaumESNG6JdfftHWrVvdNe7cuVPr1q077WP/kREjRpT6+mjVqpXHuJSUFKWlpZU6x5AhQzye//Xr12vv3r26++67FRkZ6V7eu3dvtWjRwn2oYlRUlMLDw/Xpp5/q4MGDVvUDQKDgMD8ACEAdO3ZUhw4dSiyvWbNmqYd3nWzy5Mnq06ePzjvvPLVp00Y9e/bUrbfeWqYg9uuvv6pu3bqqXr26x/KWLVu615/4b0hIiFJSUjzGNW3a9A/nPnWsJB04cECTJk3SokWLtHfvXo91hw8fLjG+QYMGHrdr1KihyMhIJSQklFh+6nlXr732mp5++mn973//8zhcsrS6TnZim5s1a+axvFatWqpZs2aJsZ06dSoxx8nPX5s2bfSXv/xFn3zyiTp27KimTZuqR48eGjhwoC677LLT1nJCs2bNlJqaesZxp9u2U9ed2M6TDwU8oUWLFvr8888l/X7Bk+nTp2vs2LFKTEzUJZdcomuvvVa33XabkpKSylQ/AAQKPpkCgEqma9eu+umnn/Tqq6+qTZs2evnll3XRRRfp5ZdfrtC6SruK3M0336x//OMfuuuuu7R48WItW7bM/amXy+UqMb5KlSplWibJ44IZb7zxhoYOHaomTZrolVde0dKlS5Wenq6rr7661MfxtZYtWyozM1OLFi1Sly5d9O6776pLly6aMGGCVx+ntOe8LOvOZPTo0dq2bZumTp2qyMhIPfbYY2rZsqU2btxoPScA+CPCFABUQnFxcRo2bJgWLlyoHTt26IILLvC4wt6pF3Y4oWHDhtq9e7eOHDnisfx///ufe/2J/7pcLm3fvt1j3I8//ljmGg8ePKjly5fr4Ycf1qRJk9S3b191795djRs3LvMcZfXOO++ocePGWrx4sW699ValpaUpNTVVBQUFZ7zviW0+9bDJ3377rcRhbg0bNlRmZmaJOU59/iSpatWq6t+/v+bNm6esrCz17t1bTzzxRJlq8oUTtZVWf2ZmpkftktSkSRONHTtWy5Yt0+bNm3X8+PESV1EEgEBHmAKASubUw9uqVaumpk2bqrCw0L3sxHc8HTp0yGNsr169VFxcrOeee85j+TPPPCOHw6FrrrlGktzn4Tz//PMe42bPnl3mOk98omROueT6rFmzyjzH2TzW2rVrtWbNmjPeNzU1VWFhYZo9e7bH/Uurs1evXvr666895j127JheeuklNWrUyH1O06k9Cg8PV6tWrWSMKdMVG32hQ4cOql27tubOnevxWlmyZIkyMjLcV2rMy8srEfiaNGmi6tWre9wPAIIB50wBQCXTqlUrXXnllWrfvr3i4uK0fv16vfPOO7rnnnvcY9q3by9JGjVqlNLS0lSlShUNGDBA1113na666io98sgj+uWXX9S2bVstW7ZMH3zwgUaPHq0mTZq479+vXz/NmjVL+/fvd18afdu2bZL++JOvk8XExKhr166aMWOGnE6n6tWrp2XLlpX4tMsbrr32Wi1evFh9+/ZV7969tX37ds2dO1etWrXS0aNHT3vfWrVq6YEHHtDUqVN17bXXqlevXtq4caOWLFlS4lythx9+WAsXLtQ111yjUaNGKS4uTq+99pq2b9+ud9991/2luD169FBSUpIuu+wyJSYmKiMjQ88995x69+5d4ny10nzzzTd64403Sixv0qSJOnfuXI5n5v8JCwvT9OnTNWzYMF1xxRW65ZZb3JdGb9Soke6//35J0rZt29StWzfdfPPNatWqlUJDQ/Xee+8pJydHAwYMsHpsAPBbFXglQQBAOZ24NPq6detKXX/FFVec8dLoU6ZMMR07djSxsbEmKirKtGjRwjzxxBPm+PHj7jFFRUXm3nvvNbVq1TIOh8PjMulHjhwx999/v6lbt64JCwszzZo1M0899ZTHZcGNMebYsWNm5MiRJi4uzlSrVs3ccMMNJjMz00jyuFT5icua//bbbyW2Z+fOnaZv374mNjbW1KhRw9x0001m9+7df3h59VPnGDJkiKlateoZnyeXy2WefPJJ07BhQxMREWEuvPBC8+GHH5ohQ4aYhg0blvpcn6y4uNhMmjTJ1KlTx0RFRZkrr7zSbN68ucRzb4wxP/30k/nTn/5kYmNjTWRkpOnYsaP58MMPPca8+OKLpmvXriY+Pt5ERESYJk2amAcffNAcPnz4tHWc6dLoJ9fSsGHDUi+Rf+LS6G+//Xapj/Hmm2+aCy+80ERERJi4uDgzaNAgs3PnTvf6ffv2mZEjR5oWLVqYqlWrmho1aphOnTqZt9566wzPIgAEHocxpxw/AQCAj3z77be68MIL9cYbb2jQoEEVXQ4AAGeFc6YAAD6Rn59fYtmsWbMUEhKirl27VkBFAAB4F+dMAQB8YsaMGdqwYYOuuuoqhYaGasmSJVqyZIlGjBih5OTkii4PAICzxmF+AACfSE9P16RJk7R161YdPXpUDRo00K233qpHHnlEoaH8vzwAQOAjTAEAAACABc6ZAgAAAAALhCkAAAAAsMBB65JcLpd2796t6tWrl+mLJAEAAAAEJ2OMjhw5orp167q/TP2PEKYk7d69mytLAQAAAHDbsWOH6tevf9oxhClJ1atXl/T7ExYTE1PB1XiX0+nUsmXL1KNHD4WFhVV0OSgHeheY6FvgoneBib4FLnoXmCpD33Jzc5WcnOzOCKdDmJLch/bFxMQEZZiKjo5WTExM0L7ggxW9C0z0LXDRu8BE3wIXvQtMlalvZTn9hwtQAAAAAIAFwhQAAAAAWCBMAQAAAIAFwhQAAAAAWCBMAQAAAIAFwhQAAAAAWCBMAQAAAIAFwhQAAAAAWCBMAQAAAIAFwhQAAAAAWCBMAQAAAIAFwhQAAAAAWCBMAQAAAIAFwhQAAAAAWCBMAQAAAICFCg9Tq1ev1nXXXae6devK4XDo/fff91hvjNH48eNVp04dRUVFKTU1VT/88IPHmAMHDmjQoEGKiYlRbGyshg8frqNHj57DrQAAAABQ2VR4mDp27Jjatm2rOXPmlLp+xowZevbZZzV37lytXbtWVatWVVpamgoKCtxjBg0apC1btig9PV0ffvihVq9erREjRpyrTQAAAABQCYVWdAHXXHONrrnmmlLXGWM0a9YsPfroo+rTp48k6fXXX1diYqLef/99DRgwQBkZGVq6dKnWrVunDh06SJJmz56tXr166W9/+5vq1q17zrYFAAAAQOVR4WHqdLZv367s7Gylpqa6l9WoUUOdOnXSmjVrNGDAAK1Zs0axsbHuICVJqampCgkJ0dq1a9W3b98S8xYWFqqwsNB9Ozc3V5LkdDrldDp9uEXn3ontCbbtqgwqS+927typ/fv3+2z++Ph41a9f32fzn6qy9C0Y0bvARN8CF70LTJWhb+XZNr8OU9nZ2ZKkxMREj+WJiYnuddnZ2apdu7bH+tDQUMXFxbnHnGrq1KmaNGlSieXLli1TdHS0N0r3O+np6RVdAizRu7Oza9cufffdd+f8celb4KJ3gYm+BS56F5iCuW95eXllHuvXYcpXxo0bpzFjxrhv5+bmKjk5WT169FBMTEwFVuZ9TqdT6enp6t69u8LCwiq6HJRDZejdpk2b1LVrV8X1vFdhcfW8Pr/zwC4dWDpbq1evVtu2bb0+f6mPWQn6FqzoXWCib4GL3gWmytC3E0etlYVfh6mkpCRJUk5OjurUqeNenpOTo3bt2rnH7N271+N+RUVFOnDggPv+p4qIiFBERESJ5WFhYUH7ogjmbQt2wdy7kJAQ5efnqzimrkITmnh9/uIio/z8fIWEhJzz5zCY+xbs6F1gom+Bi94FpmDuW3m2q8Kv5nc6KSkpSkpK0vLly93LcnNztXbtWnXu3FmS1LlzZx06dEgbNmxwj1mxYoVcLpc6dep0zmsGAAAAUDlU+CdTR48e1Y8//ui+vX37dn377beKi4tTgwYNNHr0aE2ZMkXNmjVTSkqKHnvsMdWtW1c33HCDJKlly5bq2bOn7rjjDs2dO1dOp1P33HOPBgwYwJX8AAAAAPhMhYep9evX66qrrnLfPnEu05AhQzR//nw99NBDOnbsmEaMGKFDhw6pS5cuWrp0qSIjI933+de//qV77rlH3bp1U0hIiPr166dnn332nG8LAAAAgMqjwsPUlVdeKWPMH653OByaPHmyJk+e/Idj4uLitGDBAl+UBwAAAACl8utzpgAAAADAXxGmAAAAAMACYQoAAAAALBCmAAAAAMACYQoAAAAALBCmAAAAAMACYQoAAAAALBCmAAAAAMACYQoAAAAALBCmAAAAAMACYQoAAAAALBCmAAAAAMACYQoAAAAALBCmAAAAAMACYQoAAAAALBCmAAAAAMACYQoAAAAALBCmAAAAAMACYQoAAAAALBCmAAAAAMACYQoAAAAALBCmAAAAAMACYQoAAAAALIRWdAEA4GsZGRk+mTchIUENGjTwydwAAMD/EaYABK3iowclh0ODBw/2yfyRUdHK/F8GgQoAgEqKMAUgaLkKj0rGKP7asQqLT/bq3M79O7T/w6e1b98+whQAAJUUYQpA0AuLT1ZEUtOKLgMAAAQZLkABAAAAABYIUwAAAABggTAFAAAAABYIUwAAAABggTAFAAAAABYIUwAAAABggTAFAAAAABYIUwAAAABggTAFAAAAABYIUwAAAABggTAFAAAAABYIUwAAAABggTAFAAAAABYIUwAAAABggTAFAAAAABYIUwAAAABggTAFAAAAABYIUwAAAABggTAFAAAAABYIUwAAAABggTAFAAAAABYIUwAAAABggTAFAAAAABYIUwAAAABggTAFAAAAABYIUwAAAABggTAFAAAAABYIUwAAAABggTAFAAAAABYIUwAAAABggTAFAAAAABYIUwAAAABggTAFAAAAABYIUwAAAABggTAFAAAAABYIUwAAAABggTAFAAAAABYIUwAAAABggTAFAAAAABYIUwAAAABggTAFAAAAABYIUwAAAABggTAFAAAAABYIUwAAAABggTAFAAAAABYIUwAAAABggTAFAAAAABYIUwAAAABggTAFAAAAABYIUwAAAABgwe/DVHFxsR577DGlpKQoKipKTZo00eOPPy5jjHuMMUbjx49XnTp1FBUVpdTUVP3www8VWDUAAACAYOf3YWr69Ol64YUX9NxzzykjI0PTp0/XjBkzNHv2bPeYGTNm6Nlnn9XcuXO1du1aVa1aVWlpaSooKKjAygEAAAAEs9CKLuBMvvzyS/Xp00e9e/eWJDVq1EgLFy7U119/Len3T6VmzZqlRx99VH369JEkvf7660pMTNT777+vAQMGVFjtAAAAAIKX34epSy+9VC+99JK2bdum8847T5s2bdLnn3+umTNnSpK2b9+u7Oxspaamuu9To0YNderUSWvWrCk1TBUWFqqwsNB9Ozc3V5LkdDrldDp9vEXn1ontCbbtqgwqQ+9cLpeioqIUGepQeBVz5juUU1FYFZ/N7wh1KCoqSi6Xy6NHlaFvwYreBSb6FrjoXWCqDH0rz7Y5zMknH/khl8ulv/71r5oxY4aqVKmi4uJiPfHEExo3bpyk3z+5uuyyy7R7927VqVPHfb+bb75ZDodDb775Zok5J06cqEmTJpVYvmDBAkVHR/tuYwAAAAD4tby8PA0cOFCHDx9WTEzMacf6/SdTb731lv71r39pwYIFat26tb799luNHj1adevW1ZAhQ6zmHDdunMaMGeO+nZubq+TkZPXo0eOMT1igcTqdSk9PV/fu3RUWFlbR5aAcKkPvNm3apK5duypx4DSFJzb2+vzHMj7TgaWzfTL/8ZyflbPgYa1evVpt27Z1L68MfQtW9C4w0bfARe8CU2Xo24mj1srC78PUgw8+qIcffth9uN7555+vX3/9VVOnTtWQIUOUlJQkScrJyfH4ZConJ0ft2rUrdc6IiAhFRESUWB4WFha0L4pg3rZgF8y9CwkJUX5+vgqKjEyxw+vzFziLfTZ/YZFRfn6+QkJCSu1PMPct2NG7wETfAhe9C0zB3LfybJffX80vLy9PISGeZVapUkUul0uSlJKSoqSkJC1fvty9Pjc3V2vXrlXnzp3Paa0AAAAAKg+//2Tquuuu0xNPPKEGDRqodevW2rhxo2bOnKnbb79dkuRwODR69GhNmTJFzZo1U0pKih577DHVrVtXN9xwQ8UWDwAAACBo+X2Ymj17th577DHdfffd2rt3r+rWras777xT48ePd4956KGHdOzYMY0YMUKHDh1Sly5dtHTpUkVGRlZg5QAAAACCmd+HqerVq2vWrFmaNWvWH45xOByaPHmyJk+efO4KAwAAAFCp+f05UwAAAADgjwhTAAAAAGCBMAUAAAAAFghTAAAAAGCBMAUAAAAAFghTAAAAAGCBMAUAAAAAFghTAAAAAGCBMAUAAAAAFghTAAAAAGCBMAUAAAAAFghTAAAAAGCBMAUAAAAAFghTAAAAAGCBMAUAAAAAFghTAAAAAGCBMAUAAAAAFghTAAAAAGCBMAUAAAAAFghTAAAAAGCBMAUAAAAAFghTAAAAAGCBMAUAAAAAFghTAAAAAGCBMAUAAAAAFghTAAAAAGCBMAUAAAAAFghTAAAAAGCBMAUAAAAAFghTAAAAAGCBMAUAAAAAFghTAAAAAGCBMAUAAAAAFghTAAAAAGCBMAUAAAAAFghTAAAAAGCBMAUAAAAAFghTAAAAAGCBMAUAAAAAFghTAAAAAGCBMAUAAAAAFghTAAAAAGCBMAUAAAAAFghTAAAAAGCBMAUAAAAAFghTAAAAAGCBMAUAAAAAFghTAAAAAGCBMAUAAAAAFghTAAAAAGCBMAUAAAAAFghTAAAAAGCBMAUAAAAAFghTAAAAAGCBMAUAAAAAFghTAAAAAGCBMAUAAAAAFghTAAAAAGCBMAUAAAAAFghTAAAAAGCBMAUAAAAAFghTAAAAAGCBMAUAAAAAFghTAAAAAGCBMAUAAAAAFghTAAAAAGCBMAUAAAAAFghTAAAAAGCBMAUAAAAAFghTAAAAAGCBMAUAAAAAFghTAAAAAGCBMAUAAAAAFghTAAAAAGCBMAUAAAAAFghTAAAAAGCBMAUAAAAAFgIiTO3atUuDBw9WfHy8oqKidP7552v9+vXu9cYYjR8/XnXq1FFUVJRSU1P1ww8/VGDFAAAAAIKd34epgwcP6rLLLlNYWJiWLFmirVu36umnn1bNmjXdY2bMmKFnn31Wc+fO1dq1a1W1alWlpaWpoKCgAisHAAAAEMxCK7qAM5k+fbqSk5M1b94897KUlBT3v40xmjVrlh599FH16dNHkvT6668rMTFR77//vgYMGHDOawYAAAAQ/M4qTK1fv15vvfWWsrKydPz4cY91ixcvPqvCTvj3v/+ttLQ03XTTTVq1apXq1aunu+++W3fccYckafv27crOzlZqaqr7PjVq1FCnTp20Zs2aUsNUYWGhCgsL3bdzc3MlSU6nU06n0yt1+4sT2xNs21UZVIbeuVwuRUVFKTLUofAqxuvzF4VV8dn8jlCHoqKi5HK5PHpUGfoWrOhdYKJvgYveBabK0LfybJvDGGP1F8aiRYt02223KS0tTcuWLVOPHj20bds25eTkqG/fvh6fJJ2NyMhISdKYMWN00003ad26dbrvvvs0d+5cDRkyRF9++aUuu+wy7d69W3Xq1HHf7+abb5bD4dCbb75ZYs6JEydq0qRJJZYvWLBA0dHRXqkbAAAAQODJy8vTwIEDdfjwYcXExJx2rHWYuuCCC3TnnXdq5MiRql69ujZt2qSUlBTdeeedqlOnTqlhxUZ4eLg6dOigL7/80r1s1KhRWrdundasWWMVpkr7ZCo5OVn79u074xMWaJxOp9LT09W9e3eFhYVVdDkoh8rQu02bNqlr165KHDhN4YmNvT7/sYzPdGDpbJ/MfzznZ+UseFirV69W27Zt3csrQ9+CFb0LTPQtcNG7wFQZ+pabm6uEhIQyhSnrw/x++ukn9e7dW9LvgefYsWNyOBy6//77dfXVV3stTNWpU0etWrXyWNayZUu9++67kqSkpCRJUk5OjkeYysnJUbt27UqdMyIiQhERESWWh4WFBe2LIpi3LdgFc+9CQkKUn5+vgiIjU+zw+vwFzmKfzV9YZJSfn6+QkJBS+xPMfQt29C4w0bfARe8CUzD3rTzbZX01v5o1a+rIkSOSpHr16mnz5s2SpEOHDikvL8922hIuu+wyZWZmeizbtm2bGjZsKOn3i1EkJSVp+fLl7vW5ublau3atOnfu7LU6AAAAAOBk1p9Mde3aVenp6Tr//PN100036b777tOKFSuUnp6ubt26ea3A+++/X5deeqmefPJJ3Xzzzfr666/10ksv6aWXXpIkORwOjR49WlOmTFGzZs2UkpKixx57THXr1tUNN9zgtToAAAAA4GTWYeq5555zf4/TI488orCwMH355Zfq16+fHn30Ua8VePHFF+u9997TuHHjNHnyZKWkpGjWrFkaNGiQe8xDDz2kY8eOacSIETp06JC6dOmipUuXui9eAQAAAADeZh2m4uLi3P8OCQnRww8/7JWCSnPttdfq2muv/cP1DodDkydP1uTJk31WAwAAAACcrFxhKjc3131FixPfzfRHgu2qeAAAAABwsnKFqZo1a2rPnj2qXbu2YmNj5XCUvDqWMUYOh0PFxcVeKxIAAAAA/E25wtSKFSvch/etXLnSJwUBAAAAQCAoV5i64oorSv03AAAAAFQ21t8zNW/ePL399tsllr/99tt67bXXzqooAAAAAPB31mFq6tSpSkhIKLG8du3aevLJJ8+qKAAAAADwd9ZhKisrSykpKSWWN2zYUFlZWWdVFAAAAAD4O+swVbt2bX333Xcllm/atEnx8fFnVRQAAAAA+DvrMHXLLbdo1KhRWrlypYqLi1VcXKwVK1bovvvu04ABA7xZIwAAAAD4nXJdze9kjz/+uH755Rd169ZNoaG/T+NyuXTbbbdxzhQAAACAoGcdpsLDw/Xmm2/q8ccf16ZNmxQVFaXzzz9fDRs29GZ9AAAAAOCXrMPUCeedd57OO+88b9QCAAAAAAHDOkwVFxdr/vz5Wr58ufbu3SuXy+WxfsWKFWddHAAAAAD4K+swdd9992n+/Pnq3bu32rRpI4fD4c26AAAAAMCvWYepRYsW6a233lKvXr28WQ8AAAAABATrS6OHh4eradOm3qwFAAAAAAKGdZgaO3as/v73v8sY4816AAAAACAgWB/m9/nnn2vlypVasmSJWrdurbCwMI/1ixcvPuviAAAAAMBfWYep2NhY9e3b15u1AAAAAEDAsA5T8+bN82YdAAAAABBQrM+ZkqSioiJ98sknevHFF3XkyBFJ0u7du3X06FGvFAcAAAAA/qrcn0y5XC6FhITo119/Vc+ePZWVlaXCwkJ1795d1atX1/Tp01VYWKi5c+f6ol4AAAAA8Avl+mTq+++/V9euXSX9/qW9HTp00MGDBxUVFeUe07dvXy1fvty7VQIAAACAnynzJ1PvvPOOJk+erDfeeEOS9Nlnn+nLL79UeHi4x7hGjRpp165d3q0SAAAAAPxMmT+ZcrlcKi4ulsPh8Lh9qp07d6p69ereqxAAAAAA/FCZw9TNN9+sf/7znxoxYoQkqXv37po1a5Z7vcPh0NGjRzVhwgT16tXL64UCAAAAgD8p1wUoLrroIn322WeSpJkzZyotLU2tWrVSQUGBBg4cqB9++EEJCQlauHChT4oFAAAAAH9R7qv5hYb+fpf69etr06ZNWrRokb777jsdPXpUw4cP16BBgzwuSAEAAAAAwcj6S3ul34PV4MGDvVULAAAAAAQM6zD1+uuvn3b9bbfdZjs1AAAAAPg96zB13333edx2Op3Ky8tTeHi4oqOjCVMAAAAAglq5vrT3ZAcPHvT4OXr0qDIzM9WlSxcuQAEAAAAg6FmHqdI0a9ZM06ZNK/GpFQAAAAAEG6+GKen3i1Ls3r3b29MCAAAAgF+xPmfq3//+t8dtY4z27Nmj5557TpdddtlZFwYAAAAA/sw6TN1www0etx0Oh2rVqqWrr75aTz/99NnWBQAAAAB+zTpMuVwub9YBAAAAAAHF6+dMAQAAAEBlYP3J1JgxY8o8dubMmbYPAwAAAAB+yTpMbdy4URs3bpTT6VTz5s0lSdu2bVOVKlV00UUXucc5HI6zrxIAAAAA/Ix1mLruuutUvXp1vfbaa6pZs6ak37/Id9iwYbr88ss1duxYrxUJAAAAAP7G+pypp59+WlOnTnUHKUmqWbOmpkyZwtX8AAAAAAQ96zCVm5ur3377rcTy3377TUeOHDmrogAAAADA31mHqb59+2rYsGFavHixdu7cqZ07d+rdd9/V8OHDdeONN3qzRgAAAADwO9bnTM2dO1cPPPCABg4cKKfT+ftkoaEaPny4nnrqKa8VCAAAAAD+yDpMRUdH6/nnn9dTTz2ln376SZLUpEkTVa1a1WvFAQAAAIC/Ousv7d2zZ4/27NmjZs2aqWrVqjLGeKMuAAAAAPBr1mFq//796tatm8477zz16tVLe/bskSQNHz6cy6IDAAAACHrWYer+++9XWFiYsrKyFB0d7V7ev39/LV261CvFAQAAAIC/sj5natmyZfr4449Vv359j+XNmjXTr7/+etaFAQAAAIA/s/5k6tixYx6fSJ1w4MABRUREnFVRAAAAAODvrMPU5Zdfrtdff9192+FwyOVyacaMGbrqqqu8UhwAAAAA+Cvrw/xmzJihbt26af369Tp+/LgeeughbdmyRQcOHNAXX3zhzRoBAAAAwO9YfzLVpk0bbdu2TV26dFGfPn107Ngx3Xjjjdq4caOaNGnizRoBAAAAwO9YfTLldDrVs2dPzZ07V4888oi3awIAAAAAv2f1yVRYWJi+++47b9cCAAAAAAHD+jC/wYMH65VXXvFmLQAAAAAQMKwvQFFUVKRXX31Vn3zyidq3b6+qVat6rJ85c+ZZFwcAAAAA/qrcYernn39Wo0aNtHnzZl100UWSpG3btnmMcTgc3qkOAAAAAPxUucNUs2bNtGfPHq1cuVKS1L9/fz377LNKTEz0enEAAAAA4K/Kfc6UMcbj9pIlS3Ts2DGvFQQAAAAAgcD6AhQnnBquAAAAAKAyKHeYcjgcJc6J4hwpAAAAAJVNuc+ZMsZo6NChioiIkCQVFBTorrvuKnE1v8WLF3unQgAAAADwQ+UOU0OGDPG4PXjwYK8VAwAAAACBotxhat68eb6oAwAAAAACyllfgAIAAAAAKiPCFAAAAABYIEwBAAAAgAXCFAAAAABYIEwBAAAAgAXCFAAAAABYIEwBAAAAgAXCFAAAAABYIEwBAAAAgAXCFAAAAABYIEwBAAAAgIWAC1PTpk2Tw+HQ6NGj3csKCgo0cuRIxcfHq1q1aurXr59ycnIqrkgAAAAAQS+gwtS6dev04osv6oILLvBYfv/99+s///mP3n77ba1atUq7d+/WjTfeWEFVAgAAAKgMAiZMHT16VIMGDdI//vEP1axZ07388OHDeuWVVzRz5kxdffXVat++vebNm6cvv/xSX331VQVWDAAAACCYhVZ0AWU1cuRI9e7dW6mpqZoyZYp7+YYNG+R0OpWamupe1qJFCzVo0EBr1qzRJZdcUmKuwsJCFRYWum/n5uZKkpxOp5xOpw+34tw7sT3Btl2VQWXoncvlUlRUlCJDHQqvYrw+f1FYFZ/N7wh1KCoqSi6Xy6NHlaFvwYreBSb6FrjoXWCqDH0rz7YFRJhatGiRvvnmG61bt67EuuzsbIWHhys2NtZjeWJiorKzs0udb+rUqZo0aVKJ5cuWLVN0dLRXavY36enpFV0CLAV77xYuXPj//6vY+5N3vFQacqmP5m8oXbdQu3bt0q5du0qsDfa+BTN6F5joW+Cid4EpmPuWl5dX5rF+H6Z27Nih++67T+np6YqMjPTKnOPGjdOYMWPct3Nzc5WcnKwePXooJibGK4/hL5xOp9LT09W9e3eFhYVVdDkoh8rQu02bNqlr165KHDhN4YmNvT7/sYzPdGDpbJ/MfzznZ+UseFirV69W27Zt3csrQ9+CFb0LTPQtcNG7wFQZ+nbiqLWy8PswtWHDBu3du1cXXXSRe1lxcbFWr16t5557Th9//LGOHz+uQ4cOeXw6lZOTo6SkpFLnjIiIUERERInlYWFhQfuiCOZtC3bB3LuQkBDl5+eroMjIFDu8Pn+Bs9hn8xcWGeXn5yskJKTU/gRz34IdvQtM9C1w0bvAFMx9K892+X2Y6tatm77//nuPZcOGDVOLFi30l7/8RcnJyQoLC9Py5cvVr18/SVJmZqaysrLUuXPniigZAAAAQCXg92GqevXqatOmjceyqlWrKj4+3r18+PDhGjNmjOLi4hQTE6N7771XnTt3LvXiEwAAAADgDX4fpsrimWeeUUhIiPr166fCwkKlpaXp+eefr+iyAAAAAASxgAxTn376qcftyMhIzZkzR3PmzKmYggAAAABUOgHzpb0AAAAA4E8IUwAAAABggTAFAAAAABYIUwAAAABggTAFAAAAABYIUwAAAABggTAFAAAAABYIUwAAAABggTAFAAAAABYIUwAAAABggTAFAAAAABYIUwAAAABggTAFAAAAABYIUwAAAABggTAFAAAAABYIUwAAAABggTAFAAAAABYIUwAAAABggTAFAAAAABYIUwAAAABggTAFAAAAABYIUwAAAABggTAFAAAAABYIUwAAAABggTAFAAAAABYIUwAAAABggTAFAAAAABYIUwAAAABggTAFAAAAABYIUwAAAABggTAFAAAAABYIUwAAAABggTAFAAAAABYIUwAAAABggTAFAAAAABYIUwAAAABggTAFAAAAABYIUwAAAABggTAFAAAAABYIUwAAAABggTAFAAAAABYIUwAAAABggTAFAAAAABYIUwAAAABggTAFAAAAABYIUwAAAABggTAFAAAAABYIUwAAAABggTAFAAAAABYIUwAAAABggTAFAAAAABYIUwAAAABggTAFAAAAABYIUwAAAABggTAFAAAAABYIUwAAAABggTAFAAAAABYIUwAAAABggTAFAAAAABYIUwAAAABggTAFAAAAABYIUwAAAABggTAFAAAAABYIUwAAAABggTAFAAAAABYIUwAAAABggTAFAAAAABYIUwAAAABggTAFAAAAABYIUwAAAABggTAFAAAAABYIUwAAAABggTAFAAAAABYIUwAAAABggTAFAAAAABYIUwAAAABggTAFAAAAABb8PkxNnTpVF198sapXr67atWvrhhtuUGZmpseYgoICjRw5UvHx8apWrZr69eunnJycCqoYAAAAQGXg92Fq1apVGjlypL766iulp6fL6XSqR48eOnbsmHvM/fffr//85z96++23tWrVKu3evVs33nhjBVYNAAAAINiFVnQBZ7J06VKP2/Pnz1ft2rW1YcMGde3aVYcPH9Yrr7yiBQsW6Oqrr5YkzZs3Ty1bttRXX32lSy65pCLKBgAAABDk/D5Mnerw4cOSpLi4OEnShg0b5HQ6lZqa6h7TokULNWjQQGvWrCk1TBUWFqqwsNB9Ozc3V5LkdDrldDp9Wf45d2J7gm27KoPK0DuXy6WoqChFhjoUXsV4ff6isCo+m98R6lBUVJQyMjLkcrncy0/8e+PGjQoJObsP/+Pj41W/fv2zmgNlVxnec8GIvgUueheYKkPfyrNtDmOM9/+C8RGXy6Xrr79ehw4d0ueffy5JWrBggYYNG+YRjiSpY8eOuuqqqzR9+vQS80ycOFGTJk0qsXzBggWKjo72TfEAAAAA/F5eXp4GDhyow4cPKyYm5rRjA+qTqZEjR2rz5s3uIGVr3LhxGjNmjPt2bm6ukpOT1aNHjzM+YYHG6XQqPT1d3bt3V1hYWEWXg3KoDL3btGmTunbtqsSB0xSe2Njr8x/L+EwHls72yfwn5o7rea/C4uq5l0eEOjT9mgb6y5IsFRbZ/78q54FdOrB0tlavXq22bdt6o2ScQWV4zwUj+ha46F1gqgx9O3HUWlkETJi655579OGHH2r16tUeh70kJSXp+PHjOnTokGJjY93Lc3JylJSUVOpcERERioiIKLE8LCwsaF8UwbxtwS6YexcSEqL8/HwVFBmZYofX5y9wFvts/hNzF8fUVWhCE/dyU8VIKpaJTzmrxywuMsrPz1dISEjQ9t9fBfN7LpjRt8BF7wJTMPetPNvl91fzM8bonnvu0XvvvacVK1YoJSXFY3379u0VFham5cuXu5dlZmYqKytLnTt3PtflAgAAAKgk/P6TqZEjR2rBggX64IMPVL16dWVnZ0uSatSooaioKNWoUUPDhw/XmDFjFBcXp5iYGN17773q3LkzV/IDAAAA4DN+H6ZeeOEFSdKVV17psXzevHkaOnSoJOmZZ55RSEiI+vXrp8LCQqWlpen5558/x5UCAAAAqEz8PkyV5WKDkZGRmjNnjubMmXMOKgIAAACAADhnCgAAAAD8EWEKAAAAACwQpgAAAADAAmEKAAAAACwQpgAAAADAAmEKAAAAACwQpgAAAADAAmEKAAAAACwQpgAAAADAQmhFFwDA/2VlZWnfvn1enzcjI8PrcwIAAJwrhCkAp5WVlaXmLVqqID+voksBAADwK4QpAKe1b98+FeTnKf7asQqLT/bq3Pk/r9fhz97w6pwAAADnCmEKQJmExScrIqmpV+d07t/h1fkAAADOJS5AAQAAAAAWCFMAAAAAYIEwBQAAAAAWCFMAAAAAYIEwBQAAAAAWCFMAAAAAYIEwBQAAAAAWCFMAAAAAYIEwBQAAAAAWCFMAAAAAYIEwBQAAAAAWCFMAAAAAYIEwBQAAAAAWCFMAAAAAYIEwBQAAAAAWCFMAAAAAYIEwBQAAAAAWCFMAAAAAYIEwBQAAAAAWCFMAAAAAYIEwBQAAAAAWCFMAAAAAYIEwBQAAAAAWCFMAAAAAYIEwBQAAAAAWCFMAAAAAYIEwBQAAAAAWCFMAAAAAYIEwBQAAAAAWCFMAAAAAYIEwBQAAAAAWCFMAAAAAYCG0ogsAcPaysrK0b98+n8ydkZHhk3kBAAACHWEKCHBZWVlq3qKlCvLzKroUAACASoUwBQS4ffv2qSA/T/HXjlVYfLLX58//eb0Of/aG1+cFAAAIdIQpIEiExScrIqmp1+d17t/h9TkBAACCAWEKAPyYr85ZS0hIUIMGDXwyNwAAlQVhCgD8UPHRg5LDocGDB/tk/sioaGX+L4NABQDAWSBMAYAfchUelYzxyblwzv07tP/Dp7Vv3z7CFAAAZ4EwBQB+zFfnwgEAgLPHl/YCAAAAgAXCFAAAAABYIEwBAAAAgAXCFAAAAABYIEwBAAAAgAXCFAAAAABYIEwBAAAAgAXCFAAAAABYIEwBAAAAgIXQii4AAAAApcvKytK+fft8Nn9CQoIaNGjgs/mBYEeYAgAA8ENZWVlq3qKlCvLzfPYYkVHRyvxfBoEKsESYAgAA8EP79u1TQX6e4q8dq7D4ZK/P79y/Q/s/fFr79u0jTAGWCFMAAAB+LCw+WRFJTSu6DACl4AIUAAAAAGCBT6YAAAAQcHx5cQ4uzIGyIkwBAAAgoPj64hxcmANlRZgCAABAQPHlxTm4MAfKgzAFAACAgMTFOVDRCFMAACCo8cW3p5eRkeGTeQP9eUFJWVlZ2rt3ryRp06ZNCgnx7rXsAvE1Q5gCAABBiy++/WPFRw9KDocGDx7sk/kD9XlB6U68lxwyWrhwobp27ar8/HyvPkYgvmYIUwAAIGjxxbd/zFV4VDKG845QJifeS/X6PiRJShw4TQVFxmvzB+prhjAFAACCHufW/DGeG5RHWFw9SVJ4YmOZYkcFV1PxCFN+yJvHdrtcLkmex7UWFhYqIiLCK/OfytfHuvKdEkBgCOT36s6dO3Xw4EGfzO3L/a+v5/d17eyDg9Ppzscq7W8Ub8zrLYF6Lpmv9r/n4jkPRIQpP+PtY7ujoqJKHtfqCJGMyyvzn8qXx7rynRJAYAj092r7Dhfr4IH9Ppnbl/tfn8/v49rZBweXspyPVerfKH4gkM8lOxfnCMJTUIWpOXPm6KmnnlJ2drbatm2r2bNnq2PHjhVdVrl4+9juyNDfP349cVxr/s/rdfizNwLy+Gi+UwIIDIH+XvVV7b7c//p6fl/Xzj44+JTlfKxT/0YpjxOvSV8I5HPJfLn/9eVzHsiCJky9+eabGjNmjObOnatOnTpp1qxZSktLU2ZmpmrXrl3R5ZWbt45fDq9iJBW7j2t17t/h1fkrQiDXDlQmgfxe9UXtvt7/+nL+YPjdgYpxutfMqX+jlMeJ16QvBfLr3Zf7AXjy7sXhK9DMmTN1xx13aNiwYWrVqpXmzp2r6OhovfrqqxVdGgAAAIAgFBSfTB0/flwbNmzQuHHj3MtCQkKUmpqqNWvWlBhfWFiowsJC9+3Dhw9Lkg4cOCCn0+n7gk8jNzdXkZGRcuzfLuMqPPMdzsAVKuXlJcu1Z4dMkRRyZI9X5z+Z4+BuRUZGasOGDcrNzfXq3JL0ww8/BGzt0u+vyRMn25aFy+VSXl6ePvvss9OemOvL50Xy7WvGl3P7ev4/mvvU95y35/cG3qulO/Ge4/V+bueWzq6vZ9pX+nof6cvXZLDv389mf1nRtdsK5P2v+3k58Kvy8mqd9e+5U514bnJzc7V/v4/OWy2jI0eOSJKMOfPhpw5TllF+bvfu3apXr56+/PJLde7c2b38oYce0qpVq7R27VqP8RMnTtSkSZPOdZkAAAAAAsSOHTtUv379044Jik+mymvcuHEaM2aM+7bL5dKBAwcUHx8vhyO4rpefm5ur5ORk7dixQzExMRVdDsqB3gUm+ha46F1gom+Bi94FpsrQN2OMjhw5orp1655xbFCEqYSEBFWpUkU5OTkey3NycpSUlFRifERERInvyoiNjfVliRUuJiYmaF/wwY7eBSb6FrjoXWCib4GL3gWmYO9bjRo1yjQuKC5AER4ervbt22v58uXuZS6XS8uXL/c47A8AAAAAvCUoPpmSpDFjxmjIkCHq0KGDOnbsqFmzZunYsWMaNmxYRZcGAAAAIAgFTZjq37+/fvvtN40fP17Z2dlq166dli5dqsTExIourUJFRERowoQJJQ5rhP+jd4GJvgUueheY6FvgoneBib55Coqr+QEAAADAuRYU50wBAAAAwLlGmAIAAAAAC4QpAAAAALBAmAIAAAAAC4SpAPHEE0/o0ksvVXR09B9+wXBWVpZ69+6t6Oho1a5dWw8++KCKioo8xnz66ae66KKLFBERoaZNm2r+/Pkl5pkzZ44aNWqkyMhIderUSV9//bXH+oKCAo0cOVLx8fGqVq2a+vXrV+ILk/HHGjVqJIfD4fEzbdo0jzHfffedLr/8ckVGRio5OVkzZswoMc/bb7+tFi1aKDIyUueff74++ugjj/XGGI0fP1516tRRVFSUUlNT9cMPP/h023Dm9w98Z+LEiSXeWy1atHCvL8u+y1v7UZze6tWrdd1116lu3bpyOBx6//33PdaXZf914MABDRo0SDExMYqNjdXw4cN19OhRjzHe2JfC05l6N3To0BLvw549e3qMoXfn3tSpU3XxxRerevXqql27tm644QZlZmZ6jDmX+8ig+l1pEBDGjx9vZs6cacaMGWNq1KhRYn1RUZFp06aNSU1NNRs3bjQfffSRSUhIMOPGjXOP+fnnn010dLQZM2aM2bp1q5k9e7apUqWKWbp0qXvMokWLTHh4uHn11VfNli1bzB133GFiY2NNTk6Oe8xdd91lkpOTzfLly8369evNJZdcYi699FKfbn8wadiwoZk8ebLZs2eP++fo0aPu9YcPHzaJiYlm0KBBZvPmzWbhwoUmKirKvPjii+4xX3zxhalSpYqZMWOG2bp1q3n00UdNWFiY+f77791jpk2bZmrUqGHef/99s2nTJnP99deblJQUk5+ff063tzIpy/sHvjNhwgTTunVrj/fWb7/95l5/pn2Xt/ajOLOPPvrIPPLII2bx4sVGknnvvfc81pdl/9WzZ0/Ttm1b89VXX5nPPvvMNG3a1Nxyyy3u9d7al8LTmXo3ZMgQ07NnT4/34YEDBzzG0LtzLy0tzcybN89s3rzZfPvtt6ZXr16mQYMGHn9/nKt9ZLD9riRMBZh58+aVGqY++ugjExISYrKzs93LXnjhBRMTE2MKCwuNMcY89NBDpnXr1h7369+/v0lLS3Pf7tixoxk5cqT7dnFxsalbt66ZOnWqMcaYQ4cOmbCwMPP222+7x2RkZBhJZs2aNV7ZxmDXsGFD88wzz/zh+ueff97UrFnT3TdjjPnLX/5imjdv7r598803m969e3vcr1OnTubOO+80xhjjcrlMUlKSeeqpp9zrDx06ZCIiIszChQu9tCU41ZneP/CtCRMmmLZt25a6riz7Lm/tR1E+p/5BXpb919atW40ks27dOveYJUuWGIfDYXbt2mWM8c6+FKf3R2GqT58+f3gfeucf9u7daySZVatWGWPO7T4y2H5XcphfkFizZo3OP/98jy8pTktLU25urrZs2eIek5qa6nG/tLQ0rVmzRpJ0/PhxbdiwwWNMSEiIUlNT3WM2bNggp9PpMaZFixZq0KCBewzObNq0aYqPj9eFF16op556yuMj8jVr1qhr164KDw93L0tLS1NmZqYOHjzoHnO6Xm7fvl3Z2dkeY2rUqKFOnTrRJx8py/sHvvfDDz+obt26aty4sQYNGqSsrCxJZdt3eWM/irNXlv3XmjVrFBsbqw4dOrjHpKamKiQkRGvXrnWPOdt9Kex8+umnql27tpo3b67/+7//0/79+93r6J1/OHz4sCQpLi5O0rnbRwbj70rCVJDIzs72eHFLct/Ozs4+7Zjc3Fzl5+dr3759Ki4uLnXMyXOEh4eXOG/r5DE4vVGjRmnRokVauXKl7rzzTj355JN66KGH3OvPppcnrz/5fqWNgXeV5f0D3+rUqZPmz5+vpUuX6oUXXtD27dt1+eWX68iRI2Xad3ljP4qzV5b9V3Z2tmrXru2xPjQ0VHFxcV7pJ+9Zez179tTrr7+u5cuXa/r06Vq1apWuueYaFRcXS6J3/sDlcmn06NG67LLL1KZNG0ll+/vuXP2tGWhCK7qAyuzhhx/W9OnTTzsmIyPD4wRq+Kfy9HLMmDHuZRdccIHCw8N15513aurUqYqIiPB1qUDQuuaaa9z/vuCCC9SpUyc1bNhQb731lqKioiqwMqDyGDBggPvf559/vi644AI1adJEn376qbp161aBleGEkSNHavPmzfr8888rupSgQJiqQGPHjtXQoUNPO6Zx48ZlmispKanElVBOXIElKSnJ/d9Tr8qSk5OjmJgYRUVFqUqVKqpSpUqpY06e4/jx4zp06JDH/704eUxldDa97NSpk4qKivTLL7+oefPmf9gn6cy9PHn9iWV16tTxGNOuXbsybxfKLiEh4YzvH5xbsbGxOu+88/Tjjz+qe/fuZ9x3eWM/irNXlv1XUlKS9u7d63G/oqIiHThw4Iy9OvkxzrQvxdlr3LixEhIS9OOPP6pbt270roLdc889+vDDD7V69WrVr1/fvbwsf9+dq781Aw2H+VWgWrVqqUWLFqf9Ofl44dPp3Lmzvv/+e48dVHp6umJiYtSqVSv3mOXLl3vcLz09XZ07d5YkhYeHq3379h5jXC6Xli9f7h7Tvn17hYWFeYzJzMxUVlaWe0xldDa9/PbbbxUSEuI+7KFz585avXq1nE6ne0x6erqaN2+umjVrusecrpcpKSlKSkryGJObm6u1a9dW6j75UlnePzi3jh49qp9++kl16tQp077LG/tRnL2y7L86d+6sQ4cOacOGDe4xK1askMvlUqdOndxjznZfirO3c+dO7d+/3x2M6V3FMMbonnvu0XvvvacVK1YoJSXFY/252kcG5e/Kir4CBsrm119/NRs3bjSTJk0y1apVMxs3bjQbN240R44cMcb8v8tV9ujRw3z77bdm6dKlplatWqVervLBBx80GRkZZs6cOaVerjIiIsLMnz/fbN261YwYMcLExsZ6XLnlrrvuMg0aNDArVqww69evN507dzadO3c+d09GAPvyyy/NM888Y7799lvz008/mTfeeMPUqlXL3Hbbbe4xhw4dMomJiebWW281mzdvNosWLTLR0dElLgkbGhpq/va3v5mMjAwzYcKEUi+NHhsbaz744APz3XffmT59+nBpdB8ry/sHvjN27Fjz6aefmu3bt5svvvjCpKammoSEBLN3715jzJn3Xd7aj+LMjhw54v49JsnMnDnTbNy40fz666/GmLLtv3r27GkuvPBCs3btWvP555+bZs2aeVxe21v7Ung6Xe+OHDliHnjgAbNmzRqzfft288knn5iLLrrINGvWzBQUFLjnoHfn3v/93/+ZGjVqmE8//dTjsvV5eXnuMedqHxlsvysJUwFiyJAhRlKJn5UrV7rH/PLLL+aaa64xUVFRJiEhwYwdO9Y4nU6PeVauXGnatWtnwsPDTePGjc28efNKPNbs2bNNgwYNTHh4uOnYsaP56quvPNbn5+ebu+++29SsWdNER0ebvn37mj179vhis4POhg0bTKdOnUyNGjVMZGSkadmypXnyySc9fskYY8ymTZtMly5dTEREhKlXr56ZNm1aibneeustc95555nw8HDTunVr89///tdjvcvlMo899phJTEw0ERERplu3biYzM9On24czv3/gO/379zd16tQx4eHhpl69eqZ///7mxx9/dK8vy77LW/tRnN7KlStL/Z02ZMgQY0zZ9l/79+83t9xyi6lWrZqJiYkxw4YNc/8PxhO8sS+Fp9P1Li8vz/To0cPUqlXLhIWFmYYNG5o77rijxB/J9O7cK61nkjz2X+dyHxlMvysdxhhzDj8IAwAAAICgwDlTAAAAAGCBMAUAAAAAFghTAAAAAGCBMAUAAAAAFghTAAAAAGCBMAUAAAAAFghTAAAAAGCBMAUAAAAAFghTAAAAAGCBMAUACFpDhw6Vw+Eo8dOzZ8+KLg0AEARCK7oAAAB8qWfPnpo3b57HsoiIiFLHOp1OhYWFeSw7fvy4wsPDfVYfACBw8ckUACCoRUREKCkpyeOnZs2akiSHw6EXXnhB119/vapWraonnnhCEydOVLt27fTyyy8rJSVFkZGRkqSsrCz16dNH1apVU0xMjG6++Wbl5ORU5KYBACoYYQoAUKlNnDhRffv21ffff6/bb79dkvTjjz/q3Xff1eLFi/Xtt9/K5XKpT58+OnDggFatWqX09HT9/PPP6t+/fwVXDwCoSBzmBwAIah9++KGqVavmseyvf/2r/vrXv0qSBg4cqGHDhnmsP378uF5//XXVqlVLkpSenq7vv/9e27dvV3JysiTp9ddfV+vWrbVu3TpdfPHF52BLAAD+hjAFAAhqV111lV544QWPZXFxce5/d+jQocR9GjZs6A5SkpSRkaHk5GR3kJKkVq1aKTY2VhkZGYQpAKikCFMAgKBWtWpVNW3a9LTry7IMAIBTcc4UAABn0LJlS+3YsUM7duxwL9u6dasOHTqkVq1aVWBlAICKxCdTAICgVlhYqOzsbI9loaGhSkhIKPMcqampOv/88zVo0CDNmjVLRUVFuvvuu3XFFVeUepggAKBy4JMpAEBQW7p0qerUqePx06VLl3LN4XA49MEHH6hmzZrq2rWrUlNT1bhxY7355ps+qhoAEAgcxhhT0UUAAAAAQKDhkykAAAAAsECYAgAAAAALhCkAAAAAsECYAgAAAAALhCkAAAAAsECYAgAAAAALhCkAAAAAsECYAgAAAAALhCkAAAAAsECYAgAAAAALhCkAAAAAsPD/AYddfgs1aCfBAAAAAElFTkSuQmCC"},"metadata":{}}]}]}